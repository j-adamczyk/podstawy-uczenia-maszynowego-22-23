{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasteryzacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjemy standardowych bibliotek oraz dodatkowo:\n",
    "- Plotly - do wizualizacji\n",
    "- UMAP, HDBSCAN - implementacja algorytmów\n",
    "\n",
    "**Uwaga:** upewnij się, że masz scikit-learn w wersji 1.2 lub nowszej (druga komórka poniżej). Dzięki temu będzie można ustawić opcję `transform_output=\"pandas\"`, dzięki której wszystkie transformatory w scikit-learn będą nie tylko przyjmować, ale też zwracać DataFrame, zachowując nazwy zmiennych. Będzie to bardzo przydatne w analizie danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy scipy pandas matplotlib scikit-learn missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly umap-learn hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza muzyki ze Spotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spotify w swoim API oferuje automatyczną [ekstrakcję cech z piosenek](https://developer.spotify.com/documentation/web-api/reference/get-audio-features), których można następnie użyć do wielu ciekawych analiz, jak np. [tutaj](https://rpubs.com/PeterDola/SpotifyTracks) lub [tutaj](https://rstudio-pubs-static.s3.amazonaws.com/604869_8399a2cf0e4a419da6272452c3d6a6d3.html). Datasetów jest wiele, z czego jednym z największych [jest dostępny tutaj na Kaggle](https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks?select=tracks.csv) i zawiera ponad 600 tysięcy piosenek.\n",
    "\n",
    "Cechy zwracane przez API to m. in. akustyczność, taneczność, energia, głośność czy długość piosenki. Pełna lista cech jest dostępna [w dokumentacji](https://developer.spotify.com/documentation/web-api/reference/get-audio-features). Są to bardzo dobre cechy do uczenia maszynowego - niewielka wymiarowość i tylko numeryczne. Wadą jest pewna dość mocna korelacja w niektórych podgatunkach muzycznych, ale akurat to w przypadku klasteryzacji takich danych jest całkiem ok, i może prowadzić do sensownych wniosków.\n",
    "\n",
    "Spotify definiuje i wykorzystuje ponad 125 gatunków muzycznych, które na dodatek są dość dyskusyjne i płynne. Przykładowo, według różnych klasyfikacji Powerwolf, Ensiferum i Alestorm mogą wylądować w tym samym worku, pomimo zdecydowanie różnej muzyki. Klasteryzacja pozwala tworzyć \"płynne\" tagi, klasyfikując muzykę w dużo bardziej zniuansowany sposób.\n",
    "\n",
    "**Uwaga:** zgodnie z licencją API danych nie można używać do treningu modeli ML. Oczywiście tego typu licencje nie dotyczą użytku prywatnego ani zastosowań naukowych, co wynika z ogólnych przepisów. Należy jednak pamiętać, że takich modeli nie wolno w żaden sposób publikować ani wykorzystywać komercyjnie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 1 (2.5 punktu)**\n",
    "\n",
    "1. Ściągnij pliki `artists.csv` oraz `tracks.csv` z Kaggle'a.\n",
    "2. Załaduj pliki do zmiennych `df_artists` i `df_tracks`.\n",
    "3. Usuń wiersze z wartościami brakującymi z obu ramek danych.\n",
    "4. Ponad 600 tysięcy piosenek to zdecydowanie za dużo, a na dodatek przeważająca większość z nich to drobni, mało znani artyści. Aby to zweryfikować, narysuj histogramy popularności artystów oraz piosenek. Użyj 100 kubełków (bins). Pamiętaj o tytułach wykresów.\n",
    "5. Pozostaw tylko artystów z popularnością minimum 55, oraz tylko piosenki z popularnością minimum 45.\n",
    "6. Większość piosenek trwa kilka minut, ale są anomalie, które trwają bardzo długo. Pozostaw tylko te piosenki, które trwają co najwyżej 600.000 ms (10 minut).\n",
    "7. Użyj podanej funkcji `extract_artist_id()` na ramce z piosenkami, która dodaje kolumnę `main_artist_id`.\n",
    "8. Połącz ramki w jedną, `df_songs`, z wszystkimi informacjami:\n",
    "   - użyj kolumn `main_artist_id` (ramka z piosenkami) oraz `id` (ramka z artystami)\n",
    "   - przyda się `pd.merge()`\n",
    "   - Pandas automatycznie dodaje sufiksy do kolumn (argument `suffixes`), żeby rozróżnić kolumny po takiej operacji JOIN\n",
    "   - domyślnie są to mało znaczące `_x` i `_y`, zmień je na `_track` i `_artist`\n",
    "9. Usuń duplikaty wedle kolumn `name_track` oraz `name_artist`.\n",
    "10. Mamy kolumny z danymi, które identyfikują piosenki: nazwa piosenki, artysta, gatunki muzyczne. Nie są one jednak użyteczne do samej klasteryzacji. Wyodrębnij je do osobnej ramki `df_songs_info`.\n",
    "```\n",
    "songs_info_cols = [\n",
    "    \"name_track\",\n",
    "    \"name_artist\",\n",
    "    \"genres\"\n",
    "]\n",
    "```\n",
    "11. Stwórz nową ramkę `df_songs_features`, pozostawiając tylko kolumny z cechami do klasteryzacji:\n",
    "```\n",
    "songs_features_cols = [\n",
    "    \"duration_ms\",\n",
    "    \"explicit\",\n",
    "    \"key\",\n",
    "    \"loudness\",\n",
    "    \"mode\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"time_signature\",\n",
    "]\n",
    "```\n",
    "12. Wypisz finalną liczbę piosenek (rozmiar zbioru)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def extract_artist_id(df_tracks: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_tracks[\"main_artist_id\"] = df_tracks[\"id_artists\"].apply(\n",
    "        lambda x: ast.literal_eval(x)[0]\n",
    "    )\n",
    "    return df_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy teraz przygotowane dane, więc można brać się za klasteryzację. Nasz zbiór jest dość duży, nawet po dokonanym ostrym filtrowaniu, więc potrzebujemy skalowalnych algorytmów klasteryzacji. Klasteryzacja hierarchiczna zdecydowanie odpada, pozostają więc k-means, DBSCAN i HDBSCAN. Co prawda hiperparametry do DBSCAN ciężko jest dobrać, ale zobaczymy, jak można to zrobić.\n",
    "\n",
    "Zanim przejdziemy do klasteryzacji, trzeba ustandardyzować nasze dane. Po tym trzeba przeprowadzić też globalną analizę naszego zbioru, jak na przykład rozkłady cech, albo zwizualizować go z pomocą redukcji wymiaru. Jest to ważne, bo przy późniejszej analizie klastrów musimy wiedzieć, czy klastry w ogóle różnią się od ogółu naszych danych.\n",
    "\n",
    "**Zadanie 2 (1 punkt)**\n",
    "\n",
    "1. Dokonaj standaryzacji cech, tworząc ramkę `X`.\n",
    "2. Uzupełnij kod klasy `ClusteringVisualizer`, która przyjmuje dane, numery klastrów dla poszczególnych punktów (opcjonalne), i przedstawia wykresy PCA oraz UMAP:\n",
    "   - zredukuj dane do 2 wymiarów\n",
    "   - pamiętaj o `random_state=0`\n",
    "   - [dokumentacja UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)\n",
    "   - przekaż `low_memory=False` dla UMAP\n",
    "   - ze względu na specyfikę implementacji, wykorzystanie `.fit_transform()` będzie zauważalnie szybsze od osobnych `.fit()` i `.transform()`\n",
    "   - skomentuj:\n",
    "     - czy w przypadkU PCA widać jakąś strukurę klastrów?\n",
    "     - czy widać ją w przypadku UMAP?\n",
    "     - czy redukcja nieliniowa dała lepsze, czy gorsze wyniki od liniowego PCA w wizualizacji?\n",
    "\n",
    "Czemu klasa, a nie funkcja? UMAP, jak praktycznie wszystkie metody nieliniowej redukcji wymiaru, wykorzystuje graf najbliższych sąsiadów. Jeżeli obliczymy dane po redukcji raz, i zapiszemy jako atrybut, to później będziemy mogli uniknąć liczenia tego wielokrotnie. Zapisanie tych wyników zaoszczędzi nam wiele minut :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_songs_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.decomposition import PCA\n",
    "from umap.umap_ import UMAP\n",
    "\n",
    "\n",
    "class ClusteringVisualizer:\n",
    "    def __init__(self, X: np.ndarray | pd.DataFrame):\n",
    "        # perform PCA and UMAP\n",
    "        # save results as Numpy arrays\n",
    "        self.X_2d_pca = ...\n",
    "        self.X_2d_umap = ...\n",
    "\n",
    "    def visualize(self, labels: Optional[np.ndarray] = None):\n",
    "        if labels is None:\n",
    "            labels = np.ones(len(X))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "        axes[0].scatter(self.X_2d_pca[:, 0], self.X_2d_pca[:, 1], c=labels)\n",
    "        axes[1].scatter(self.X_2d_umap[:, 0], self.X_2d_umap[:, 1], c=labels)\n",
    "\n",
    "        axes[0].title.set_text(\"PCA\")\n",
    "        axes[1].title.set_text(\"UMAP\")\n",
    "\n",
    "        fig.suptitle(\"Clustering visualization\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_visualizer = ClusteringVisualizer(X)\n",
    "clustering_visualizer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz przeanalizujemy nasz zbiór. Posłużą nam do tego poniżej zdefiniowane klasy `KMeansAnalzyer`, `DBSCANAnalyzer` i `HDBSCANAnalyzer`. Metoda `.analyze_clustering()` dla każdego klastra:\n",
    "1. Wypisuje liczbę klastrów, oraz informację o szumie (dla DBSCAN i HDBSCAN).\n",
    "2. Wypisuje wartość metryki [Calinski-Harabasz index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html), jeżeli liczba klastrów to co najmniej 2. Dla DBSCAN i HDBSCAN uwzględniamy tylko punkty z klastrów, ignorując szum.\n",
    "2. Rysuje wykres klastrów po redukcji z PCA oraz UMAP.\n",
    "3. Tworzy [wykres radowy (radar chart)](https://plotly.com/python/radar-chart/). Pozwala on wygodnie porównać wartości średnie dla poszczególnych cech.\n",
    "4. Wypisuje piosenki i artystów reprezentujące klaster:\n",
    "   - dla k-means jest to 10 punktów najbliższych do centroidu\n",
    "   - dla DBSCAN wybieramy heurystycznie 10 punktów najbliższych do centroidu\n",
    "   - dla HDBSCAN wybieramy punkty o najwyższym prawdopodobieństwie (nie używamy exemplars, bo jest to bardzo wolne [i mało wygodne](https://github.com/scikit-learn-contrib/hdbscan/issues/304))\n",
    "5. Wypisuje 10 najczęstszych gatunków muzycznych w klastrze.\n",
    "\n",
    "Dla DBSCAN i HDBSCAN, które same wykrywają liczbę klastrów i może być ich bardzo dużo, analizowane jest tylko:\n",
    "- max 10 największych klastrów\n",
    "- tylko te, które mają co najmniej 100 próbek\n",
    "\n",
    "Na początek potraktujemy cały nasz zbiór jako jeden wielki klaster dla algorytmu k-means i popatrzymy, co tam się dzieje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import Counter\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "\n",
    "class ClusteringAnalyzer(ABC):\n",
    "    def __init__(\n",
    "        self, df_songs_info: pd.DataFrame, clustering_visualizer: ClusteringVisualizer\n",
    "    ):\n",
    "        self.df_songs_info = df_songs_info\n",
    "        self.clustering_visualizer = clustering_visualizer\n",
    "\n",
    "    @abstractmethod\n",
    "    def analyze_clustering(self, X: pd.DataFrame, clustering) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_top_clusters_labels(self, labels: np.ndarray) -> np.ndarray:\n",
    "        labels = labels[labels != -1]\n",
    "        label_counts = pd.Series(labels).value_counts()\n",
    "        label_counts = label_counts[label_counts >= 100]\n",
    "        top_labels = label_counts.index[:10].values\n",
    "        return top_labels\n",
    "\n",
    "    def _create_radar_chart(self, X: pd.DataFrame, label: int) -> None:\n",
    "        fig = go.Figure()\n",
    "        vals = minmax_scale(X).mean()\n",
    "        fig.add_trace(go.Scatterpolar(r=vals, theta=X.columns, fill=\"toself\"))\n",
    "        fig.update_layout(title_text=f\"Cluster {label}\", title_x=0.5)\n",
    "        fig.update_polars(radialaxis=dict(range=[0, 1]))\n",
    "        fig.show()\n",
    "\n",
    "    def _get_central_songs(\n",
    "        self, X: pd.DataFrame, cluster_songs_info: pd.DataFrame, prototype: np.ndarray\n",
    "    ) -> None:\n",
    "        n_neighbors = min(10, len(X))\n",
    "        nn = NearestNeighbors(n_neighbors=n_neighbors, n_jobs=-1)\n",
    "        nn.fit(X.values)\n",
    "        _, nn_idxs = nn.kneighbors(prototype)\n",
    "        nn_idxs = nn_idxs.ravel()\n",
    "        nn_songs_info = cluster_songs_info.iloc[nn_idxs]\n",
    "        nn_songs_info = nn_songs_info.drop(columns=\"genres\")\n",
    "        nn_songs_info.columns = [\"Track\", \"Artist\"]\n",
    "        nn_songs_info = nn_songs_info.reset_index(drop=True)\n",
    "        display(nn_songs_info)\n",
    "\n",
    "    def _get_cluster_top_genres(\n",
    "        self,\n",
    "        cluster_songs_info: pd.DataFrame,\n",
    "    ) -> None:\n",
    "        genres = cluster_songs_info[\"genres\"].values\n",
    "        genres = [ast.literal_eval(x) for x in genres]\n",
    "        genres = Counter(itertools.chain(*genres))\n",
    "        top_genres = genres.most_common(10)\n",
    "        top_genres = [name for name, count in top_genres]\n",
    "        print(\"Top 10 genres in the cluster:\", top_genres)\n",
    "\n",
    "\n",
    "class KMeansAnalyzer(ClusteringAnalyzer):\n",
    "    def analyze_clustering(self, X: pd.DataFrame, clustering: KMeans) -> None:\n",
    "        labels = clustering.labels_\n",
    "        centroids = clustering.cluster_centers_\n",
    "\n",
    "        print(f\"Number of clusters: {max(labels) + 1}\")\n",
    "\n",
    "        if labels.max() >= 2:\n",
    "            ch_index = calinski_harabasz_score(X, labels)\n",
    "            print(f\"CH-index value: {ch_index:.2f}\")\n",
    "\n",
    "        self.clustering_visualizer.visualize(labels)\n",
    "\n",
    "        for label in range(0, max(labels) + 1):\n",
    "            X_cluster = X[labels == label]\n",
    "            cluster_songs_info = self.df_songs_info[labels == label]\n",
    "            centroid = centroids[label].reshape((1, -1))\n",
    "\n",
    "            print(f\"Cluster {label} size: {len(X_cluster)}\")\n",
    "            self._create_radar_chart(X_cluster, label)\n",
    "            self._get_central_songs(X_cluster, cluster_songs_info, centroid)\n",
    "            self._get_cluster_top_genres(cluster_songs_info)\n",
    "\n",
    "\n",
    "class DBSCANAnalyzer(ClusteringAnalyzer):\n",
    "    def analyze_clustering(self, X: pd.DataFrame, clustering: KMeans) -> None:\n",
    "        labels = clustering.labels_\n",
    "\n",
    "        noise_num = (labels == -1).sum()\n",
    "        noise_perc = noise_num / len(X)\n",
    "\n",
    "        print(f\"Number of clusters: {max(labels) + 1}\")\n",
    "        print(f\"Percentage of noise points: {100 * noise_perc:.2f}%\")\n",
    "\n",
    "        if labels.max() >= 2:\n",
    "            X_non_noise = X[labels != -1]\n",
    "            labels_non_noise = labels[labels != -1]\n",
    "            ch_index = calinski_harabasz_score(\n",
    "                X_non_noise,\n",
    "                labels_non_noise,\n",
    "            )\n",
    "            print(f\"CH-index value: {ch_index:.2f}\")\n",
    "\n",
    "        self.clustering_visualizer.visualize(labels)\n",
    "\n",
    "        for label in self._get_top_clusters_labels(labels):\n",
    "            X_cluster = X[labels == label]\n",
    "            cluster_songs_info = self.df_songs_info[labels == label]\n",
    "            centroid = self._get_cluster_centroid(X_cluster)\n",
    "\n",
    "            print(f\"Cluster {label} size: {len(X_cluster)}\")\n",
    "            self._create_radar_chart(X_cluster, label)\n",
    "            self._get_central_songs(X_cluster, cluster_songs_info, centroid)\n",
    "            self._get_cluster_top_genres(cluster_songs_info)\n",
    "\n",
    "    def _get_cluster_centroid(self, X_cluster: pd.DataFrame) -> np.ndarray:\n",
    "        centroid = np.mean(X_cluster.values, axis=0)\n",
    "        return np.array(centroid).reshape((1, -1))\n",
    "\n",
    "\n",
    "class HDBSCANAnalyzer(ClusteringAnalyzer):\n",
    "    def analyze_clustering(self, X: pd.DataFrame, clustering: HDBSCAN) -> None:\n",
    "        labels = clustering.labels_\n",
    "        probas = clustering.probabilities_\n",
    "\n",
    "        noise_num = (labels == -1).sum()\n",
    "        noise_perc = noise_num / len(X)\n",
    "\n",
    "        print(f\"Number of clusters: {max(labels) + 1}\")\n",
    "        print(f\"Percentage of noise points: {100 * noise_perc:.2f}%\")\n",
    "\n",
    "        if labels.max() >= 2:\n",
    "            X_non_noise = X[labels != -1]\n",
    "            labels_non_noise = labels[labels != -1]\n",
    "            ch_index = calinski_harabasz_score(\n",
    "                X_non_noise,\n",
    "                labels_non_noise,\n",
    "            )\n",
    "            print(f\"CH-index value: {ch_index:.2f}\")\n",
    "\n",
    "        self.clustering_visualizer.visualize(labels)\n",
    "\n",
    "        for label in self._get_top_clusters_labels(labels):\n",
    "            X_cluster = X[labels == label]\n",
    "            cluster_songs_info = self.df_songs_info[labels == label]\n",
    "            cluster_probas = probas[labels == label]\n",
    "\n",
    "            print(f\"Cluster {label} size: {len(X_cluster)}\")\n",
    "            self._create_radar_chart(X_cluster, label)\n",
    "            self._get_most_probable_songs(cluster_songs_info, cluster_probas)\n",
    "            self._get_cluster_top_genres(cluster_songs_info)\n",
    "\n",
    "    def _get_most_probable_songs(\n",
    "        self, cluster_songs_info: pd.DataFrame, cluster_probas: np.ndarray\n",
    "    ) -> None:\n",
    "        top_10_indexes = np.argpartition(cluster_probas, -10)[-10:]\n",
    "        nn_songs_info = cluster_songs_info.iloc[top_10_indexes]\n",
    "        nn_songs_info = nn_songs_info.drop(columns=\"genres\")\n",
    "        nn_songs_info.columns = [\"Track\", \"Artist\"]\n",
    "        nn_songs_info = nn_songs_info.reset_index(drop=True)\n",
    "        display(nn_songs_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "clustering = KMeans(n_clusters=1, n_init=\"auto\", random_state=0)\n",
    "clustering.fit(X)\n",
    "\n",
    "kmeans_analyzer = KMeansAnalyzer(df_songs_info, clustering_visualizer)\n",
    "kmeans_analyzer.analyze_clustering(X, clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygląda na to, że najpopularniejsze są pop i rock, a najbardziej \"typowe\" piosenki też należą do takiej grupy. Ogół piosenek ma dość dużą głośność, dużą dynamikę (wysokie time signature - \"how many beats are in each bar\") i są pozytywne (valence - \"tracks with high valence sound more positive\"). Zasadniczo jest to zgodne z intuicją. Ciekawe jednak, co będzie, jak wybierzemy więcej klastrów.\n",
    "\n",
    "**Zadanie 3 (1.5 punktu)**\n",
    "\n",
    "1. Wytrenuj k-means dla 3 klastrów.\n",
    "2. Zwizualizuj wyniki za pomocą PCA i UMAP. Jako kolor punktów (np. parametr `c` w `plt.scatter()`) przekaż numery klastrów dla punktów.\n",
    "3. Dokonaj opisu i analizy wynikowych klastrów, na przykład:\n",
    "   - czy reprezentują jakieś konkretne grupy muzyczne?\n",
    "   - na ile \"konkretne\", sensowne są klastry?\n",
    "   - czy jest widoczna separacja klastrów, czy różnią się od siebie?\n",
    "4. Dokonaj tuningu liczby klastrów:\n",
    "   - sprawdź wartości z zakresu [5, 20] - w końcu wiemy, że gatunków jest sporo, i klastrów powinno też być więcej\n",
    "   - dla każdej wartości sprawdź wartość metryki [Calinski-Harabasz index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)\n",
    "   - przedstaw wykres liniowy (line plot) wartości metryki\n",
    "   - wybierz model o najwyższej wartości metryki\n",
    "   - zwizualizuj wyniki z pomocą PCA i UMAP\n",
    "   - dokonaj interpretacji jego klastrów\n",
    "   - skomentuj:\n",
    "     - czy klastry są faktycznie lepiej interpretowalne po tuningu?\n",
    "     - czy wysokość CH-index faktycznie odpowiada jakości klasteryzacji?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem z k-means jest taki, że wykrywa tylko klastry sferyczne. W przypadku zbiorów, w których klastry mogą być mocno wymieszane, będzie to szczególnym problemem. Tak może być tutaj - w końcu mamy co najmniej kilka podgatunków rocka i popu, które są zbliżone do siebie. Mamy też dość egzotyczne podgatunki takie jak \"art rock\" czy \"rock en espanol\", które być może są nieliczne i stanowią de facto szum.\n",
    "\n",
    "Do takich sytuacji doskonale nadaje się DBSCAN. Ma co prawda większą złożoność, ale dla danych tego rozmiaru jest to jeszcze akceptowalne. Sprawdźmy, jak działa dla domyślnych hiperparametrów - epsilon 0.5 i min_samples 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(n_jobs=-1)\n",
    "clustering.fit(X)\n",
    "\n",
    "dbscan_analyzer = DBSCANAnalyzer(df_songs_info, clustering_visualizer)\n",
    "dbscan_analyzer.analyze_clustering(X, clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy niesamowicie dużo klastrów oraz szumu - niedobrze! Oznacza to, że zbyt ciężko jest \"dotrzeć\" do kolejnych punktów, żeby stworzyć klastry. Mogą być tego dwie przyczyny:\n",
    "- zbyt mały promień epsilon\n",
    "- zbyt duże min_samples\n",
    "\n",
    "Biorąc pod uwagę, że minimalna liczba próbek to zaledwie 5, to to drugie nie występuje. Musimy zatem zwiększyć promień. Ale jak, skoro nie znamy jednostek?\n",
    "\n",
    "Dobór hiperparametrów w DBSCAN dobrze opisuje [ten post](http://www.sefidian.com/2022/12/18/how-to-determine-epsilon-and-minpts-parameters-of-dbscan-clustering/), który zgadza się zresztą z \"Data Clustering\" Charu C. Aggarwala.\n",
    "\n",
    "Wartość min_samples można przyjąć z grubsza (rule-of-thumb) jako liczbę wymiarów razy 2. Bierze to poprawkę na rzadkość przestrzeni, i generalnie jest dobrą wartością wyjściową. Jeżeli mamy dużo szumu, to można próbować więcej.\n",
    "\n",
    "Przy stałym min_samples heurystyka do wyboru epsilon wygląda następująco. Obliczamy rozkład odległości do k najbliższych sąsiadów, przyjmując k = min_samples, a następnie przedstawiamy to na wykresie. Typowo będzie miał kształt z grubsza zbliżony do litery L (lub odwróconego L), i dobra wartość epsilon jest na punkcie przegięcia tego wykresu, tzw. elbow / knee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 4 (1.5 punktu)**\n",
    "\n",
    "Stwórz wykres odległości do najbliższych sąsiadów, przyjmując liczbę najbliższych sąsiadów równą 2 razy liczba cech. Może się przydać [ten tutorial](https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd).\n",
    "\n",
    "Odczytaj z wykresu wartość punktu przegięcia. Czy jest to łatwe? Czy jesteś w stanie ją wyznaczyć dokładnie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taka heurystyczna metoda jest wykorzystywana w wielu przypadkach doboru hiperparametrów w klasteryzacji. Zasadnicza idea jest taka, że punkt przegięcia odpowiada momentowi, gdzie dane zaczynają wyraźnie rozróżniać się od siebie i tworzyć wyraźne grupy. Jest to proste i szybkie, ale jest dość nieprecyzyjne w porównaniu do innych metod (jeżeli w ogóle istnieją dla danego zadania!). Jeżeli chcemy zrobić walidację skrośną, korzystając z jakiejś metryki, to taki wykres daje nam sensowny punkt wyjścia do zakresu hiperparametru.\n",
    "\n",
    "Warto też pamiętać, że klasteryzacja to proces z natury interaktywny. Takie wykresy są ważne, bo zwiększają naszą intuicję, ale np. dokładne dobranie wartości jest typowo robione ręcznie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 5 (1.5 punktu)**\n",
    "\n",
    "Wytrenuj DBSCAN z promieniem epsilon wyznaczonym z wykresu i min_samples równym 2 razy liczbie cech. W razie potrzeby zmniejsz lub zwiększ epsilon, aby uzyskać wyniki, które wydają się sensowne (liczba klastrów, procent szumu, CH-index).\n",
    "\n",
    "Dokonaj analizy wyjściowych klastrów. Czy finalna ich liczba oraz procent szumu są sensowne? Czy klastry są lepiej widoczne, lub lepiej interpretowalne, niż w przypadku k-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN ma dwie zasadnicze wady: trudny dobór hiperparametrów oraz podatność na szum. W szczególności to drugie widać, jeżeli w naszym zbiorze ciągle znajduje on dużo szumu - to oznaka, że mamy zmienną gęstość w danych i przydałoby się coś adaptacyjnego, jak OPTICS albo HDBSCAN. OPTICS jest bardziej zoptymalizowany pod pamięć niż szybkość w scikit-learn, plus ma raczej nieintuicyjne hiperparametry (może nawet bardziej, niż DBSCAN), więc wykorzystamy tutaj HDBSCAN.\n",
    "\n",
    "Jego głównym hiperparametrem jest `min_cluster_size`, czyli po prostu minimalna liczba punktów, aby stworzyć klaster. Dodatkowo `min_samples` ma takie samo znaczenie, jak w DBSCAN, czyli jest to minimalna liczba punktów, aby utworzyć punkt typu CORE. Domyślnie są one równe, ale oczywiście można ustawić `min_samples` (sporo) mniejsze od `min_cluster_size`, i dostrajać oba.\n",
    "\n",
    "Co ważne, HDBSCAN potrafi cache'ować wyniki obliczeń dla stałego `min_samples`, dlatego warto je czasem ustawiać choćby tylko z tego powodu. Caching wymaga też podania argumentu `memory`. Przyspiesza to drugie i kolejne wykonania algorytmu o rzędy wielkości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "clustering = HDBSCAN(memory=\"tmp\")\n",
    "clustering.fit(X)\n",
    "\n",
    "dbscan_analyzer = HDBSCANAnalyzer(df_songs_info, clustering_visualizer)\n",
    "dbscan_analyzer.analyze_clustering(X, clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domyślne `min_cluster_size` to zaledwie 5, a mimo tego HDBSCAN wcale nie znalazł tak dużo klastrów - pokazuje to jego zdolność do adaptacji do zbioru danych. CH-index nie jest zbyt wysoki, pomimo tego, że niektóre klastry wydają się ciekawe i inne niż w k-means. Pokazuje to też, że klastry pewnie nie są sferyczne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 6 (2 punkty)**\n",
    "\n",
    "Dokonaj ręcznego tuningu hiperparametrów `min_cluster_size` oraz `min_samples`. Pamiętaj, że przy stałym `min_samples` możesz praktycznie za darmo zmieniać `min_cluster_size` dzięki cache'owaniu.\n",
    "\n",
    "Skomentuj finalne wyniki. Czy udało się uzyskać sensowne, interpretowalne klastry? Czy są one w jakimś stopniu nieoczywiste, tj. czy otrzymaliśmy jakąś nową wiedzę dzięki klasteryzacji? Czy CH-index wskazał sensownie na jakość klasteryzacji w HDBSCAN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dodatkowe (3 punkty)\n",
    "\n",
    "Klasteryzacja hierarchiczna jest przydatnym narzędziem, dającym od razu znaczną ilość informacji co do klasteryzacji, ale ze względu na złożoność nadaje się tylko do mniejszych zbiorów danych. Takim zbiorem jest dobrze nam już znany [Ames housing](https://www.openintro.org/book/statdata/?data=ames).\n",
    "\n",
    "1. Załaduj zbiór i dokonaj odpowiedniego preprocessingu. Zwizualizuj go z pomocą PCA oraz UMAP.\n",
    "2. Zastosuj klasteryzację hierarchiczną do tego zbioru, wykorzystując metrykę euklidesową i Ward linkage.\n",
    "3. Narysuj dendrogram dla wynikowej klasteryzacji. Czy widać jakąś strukturę klastrującą? Sprawdź kilka przykładowych próbek, które wydają się podobne - czy faktycznie te domy są podobne?\n",
    "4. Wybierz klasteryzację, która wydaje się najbardziej stabilna, tj. ma najdłuższe pionowe linie na dendrogramie. Sprawdź jej CH-index, oraz zwizualizuj ją za pomocą PCA oraz UMAP. Ile ma ona klastrów? Czy wydaje się ona sensowna? Wypisz przykładowe domy z klastrów. Czy faktycznie wskazują na zróżnicowanie klastrów?\n",
    "5. Porównaj tę klasteryzację z k-means (dokonaj tuningu liczby klastrów z pomocą CH-index) oraz HDBSCAN. Ile klastrów wybrały te metody? Czy na wizualizacji PCA i UMAP dają lepsze, czy gorsze wyniki od klasteryzacji hierarchicznej?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUM",
   "language": "python",
   "name": "pum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
