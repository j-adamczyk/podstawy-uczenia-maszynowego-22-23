{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja niezbalansowana i anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poza standardowymi narzędziami do klasyfikacji tabelarycznej użyjemy bibliotek:\n",
    "1. [Imbalanced-learn](https://imbalanced-learn.org/stable/index.html) - biblioteka implementująca różne algorytmy undersamplingu i oversamplingu\n",
    "2. [PyOD](https://pyod.readthedocs.io/en/latest/index.html) - biblioteka implementująca mnóstwo algorytmów outlier detection\n",
    "3. [XGBoost](https://xgboost.readthedocs.io/en/stable/) - oficjalna implementacja algorytmu XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy scipy pandas matplotlib scikit-learn missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn pyod xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja umiarkowanie niezbalansowana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw wykorzystamy zbiór danych [Polish companies bankruptcy](https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data). Dotyczy on klasyfikacji, na podstawie danych z raportów finansowych, czy firma zbankrutuje w ciągu najbliższych kilku lat. Jest to zadanie szczególnie istotne dla banków, funduszy inwestycyjnych, firm ubezpieczeniowych itp., które z tego powodu zatrudniają licznie data scientistów. Zbiór zawiera 64 cechy, obliczone przez ekonomistów, którzy stworzyli ten zbiór, opisane na stronie UCI.\n",
    "\n",
    "Wykorzystamy podzbiór, w którym na podstawie finansowych firmy po 3 latach monitorowania chcemy przewidywać, czy firma zbankrutuje w ciągu najbliższych 3 lat. Jest to dość realistyczny horyzont czasowy, a przy tym największy z podzbiorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "data = arff.loadarff(\"polish_companies_bankruptcy_3_year_data.arff\")\n",
    "\n",
    "df = pd.DataFrame(data[0])\n",
    "y = df.pop(\"class\").astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 1 (1 punkt)**\n",
    "\n",
    "1. Zwizualizuj brakujące ilość brakujących danych z pomocą biblioteki `missingno` (funkcja `bar()`).\n",
    "2. Zwizualizuj rozkład klas na wykresie.\n",
    "3. Usuń cechę `Attr37`, mającą dużo wartości brakujących.\n",
    "4. Dokonaj podziału na zbiór treningowy i testowy w proporcjach 75%-25%, ze stratyfikacją. Pamiętaj o `random_state=0`.\n",
    "5. Zbuduj i zastosuj pipeline (`make_pipeline`) do czyszczenia danych, składający się z:\n",
    "   - uzupełnienia wartości brakujących wartością średnią (`SimpleImputer`)\n",
    "   - standaryzacji danych (`StandardScaler`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standaryzacja była szczególnie ważna, bo metody undersamplingu i oversamplingu są oparte o najbliższych sąsiadów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-sensitive learning i threshold tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako naszego algorytmu użyjemy lasu losowego (Random Forest). Dla przypomnienia, jest on oparty o **uczenie zespołowe (ensemble learning)**, w którym uśredniamy decyzje wielu klasyfikatorów bazowych. Są to drzewa decyzyjne. Losujemy w nim **próbki bootstrapowe (bootstrap samples)**, czyli losujemy z powtórzeniami tyle punktów, ile wynosi rozmiar naszego zbioru. Dla każdej losujemy także podzbiór cech, typowo tyle, ile wynosi pierwiastek kwadratowy z liczby wszystkich cech. Następnie trenujemy drzewa decyzyjne na takich wylosowanych podzbiorach. Decyzja klasyfikatora jest podejmowana przez głosowanie drzew (w klasyfikacji) lub ich uśrednienie (w regresji).\n",
    "\n",
    "W wielu zastosowaniach dużą zaletą lasów losowych jest ich niska podatność na tuning hiperparametrów, tzw. **tunability**. Algorytmy o wysokim tunability (np. SVM) są podatne na dobór hiperparametrów i wymagają jego zastosowania, żeby osiągnąć dobre wyniki. Random Forest działa typowo doskonale z domyślnymi hiperparametrami, co najwyżej warto czasem ustawić większą liczbę drzew, niż domyślna. Ciekawe artykuły w tej kwestii to:\n",
    "\n",
    "> Probst, Philipp, Anne-Laure Boulesteix, and Bernd Bischl. *\"Tunability: Importance of hyperparameters of machine learning algorithms.\"* The Journal of Machine Learning Research 20.1 (2019): 1934-1965. [link](https://www.jmlr.org/papers/volume20/18-444/18-444.pdf)\n",
    "\n",
    "> Probst, Philipp, Marvin N. Wright, and Anne‐Laure Boulesteix. *\"Hyperparameters and tuning strategies for random forest.\"* Wiley Interdisciplinary Reviews: data mining and knowledge discovery 9.3 (2019): e1301. [link](https://arxiv.org/pdf/1804.03515.pdf)\n",
    "\n",
    "Dzięki wykorzystaniu Random Forest zasadniczo nie będziemy potrzebować tuningu hiperparametrów dla klasyfikatora. Nadaje się też dobrze do klasyfikacji niezbalansowanej: drzewa decyzyjne łatwo integrują ważenie klas w proces treningu, a uśrednianie decyzji mocno zmniejsza wariancję błędu.\n",
    "\n",
    "Ze względu na niezbalansowanie zbioru, które jest znaczące, ale nie ekstremalne, wykorzystamy dwie metryki: AUROC oraz F1-score. Ta druga będzie przydatna przy **threshold tuningu**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "clf_rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_rf.predict(X_test)\n",
    "y_pred_score = clf_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred_score)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUROC: {100 * auroc:.2f}%\")\n",
    "print(f\"F1-score: {100 * f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC wydaje się niezłe, ale F1-score pozostawia wiele do życzenia. Zobaczmy, czy **cost-sensitive learning** coś zmieni. Skorzystamy z domyślnej heurystyki do ważenia klas `\"balanced\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_csl = RandomForestClassifier(class_weight=\"balanced\", random_state=0, n_jobs=-1)\n",
    "clf_rf_csl.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_rf_csl.predict(X_test)\n",
    "y_pred_score = clf_rf_csl.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred_score)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AUROC: {100 * auroc:.2f}%\")\n",
    "print(f\"F1-score: {100 * f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedna metryka rośnie, druga maleje - tak też się może zdarzyć. Takie sytuacje są zawsze ciekawe, bo pokazują różne aspekty tego, jak radzi sobie nasz klasyfikator. F1-score łączy precyzję i czułość, więc warto przeanalizować to głębiej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"RF\")\n",
    "rf_precision = precision_score(y_test, clf_rf.predict(X_test))\n",
    "rf_recall = recall_score(y_test, clf_rf.predict(X_test))\n",
    "print(f\"  Precision: {100 * rf_precision:.2f}%\")\n",
    "print(f\"  Recall: {100 * rf_recall:.2f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"RF with cost-sensitive learning\")\n",
    "rf_csl_precision = precision_score(y_test, clf_rf_csl.predict(X_test))\n",
    "rf_csl_recall = recall_score(y_test, clf_rf_csl.predict(X_test))\n",
    "print(f\"  Precision: {100 * rf_csl_precision:.2f}%\")\n",
    "print(f\"  Recall: {100 * rf_csl_recall:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z cost-sensitive learningiem predykcje prawdopodobieństwa co prawda są lepsze (bo mamy wyższy AUROC), ale i precyzja, i czułość spadły. No i w obu przypadkach mamy naprawdę niski recall!\n",
    "\n",
    "Coś trzeba z tym zrobić. Skoro F1-score to metryka binarna, to najłatwiej zmienić próg klasy pozytywnej, czyli zrobić threshold tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 2 (1.5 punktu)**\n",
    "\n",
    "Zaimplementuj threshold tuning z pomocą walidacji skrośnej. Skorzystaj z funkcji `thresholded_f1_score()`, która jest gotową metryką, obliczającą F1-score dla podanych prawdopodobieństw klasy pozytywnej i progu klasyfikacji.\n",
    "\n",
    "1. Stwórz listę progów [0.1, 0.15, 0.2, .., 0.5]\n",
    "2. Dla każdego progu stwórz nowy obiekt metryki z pomocą funkcji `make_scorer()`. Pamiętaj, że większa wartość jest lepsza i potrzebujemy prawdopodobieństw. Trzeba też podać wartość dla naszego progu (`threshold`) z pomocą `**kwargs`.\n",
    "3. Oblicz wyniki walidacji skrośnej z pomocą funkcji `cross_val_score` dla Random Forest z cost-sensitive tuning. Wykorzystaj 5-fold CV. Funkcja ta zwraca wyniki dla wszystkich foldów - oblicz średni wynik.\n",
    "4. Zwizualizuj na wykresie wyniki F1-score dla poszczególnych progów. Pamiętaj o opisaniu osi i tytule wykresu.\n",
    "5. Dla optymalnego progu oblicz i wypisz F1-score, precision i recall. Próg, dla którego osiągnięto najwyższy F1-score, można łatwo wyciągnąć z pomocą `np.argmax()`.\n",
    "6. Skomentuj zmianę w precision i recall. Czy twoim zdaniem warto dokonać takiej zmiany w przypadku tego zbioru, tj. przewidywania, czy spółka zbankrutuje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholded_f1_score(y_true, y_score, threshold: float, **kwargs) -> float:\n",
    "    y_pred = y_score >= threshold\n",
    "    return f1_score(y_true, y_pred, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "thresholds = ...\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling, oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Być może klasa większościowa, której jest 95%, jest mocno zaszumiona i są tam przykłady, które warto byłoby usunąć. Czemu tak może być?\n",
    "\n",
    "Pamiętajmy, że klasa pozytywna to spółki, które zbankrutują w ciągu najbliższych 3 lat. Przy granicy decyzyjnej w klasie dominującej mogą być na przykład startupy o dużym ryzyku, które nie zbankrutowały, ale było to kwestią dobrej koniunktury i szczęśliwego trafu tych spółek. Równie dobrze mogłyby upaść przez niskie zasoby twarde czy rosnące koszty. Można je potraktować jak mało miarodajny szum, który tylko z przyczyn dość losowych nie stał się klasą pozytywną (tj. spółkami, które zamknęły działalność).\n",
    "\n",
    "Dla uproszczenia w tym i dalszych zadaniach skorzystamy z funkcji `assess_rf_performance()`, żeby łatwo sprawdzać AUROC i F1-score klasyfikatorów.\n",
    "\n",
    "Najpierw zastosujemy algorytm Edited Nearest Neighbors (ENN) z domyślnymi parametrami: \n",
    "- `k=3`\n",
    "- `kind_sel=\"all\"` (wszyscy sąsiedzi muszą być z klasy dominującej, aby punkt pozostał w zbiorze)\n",
    "\n",
    "Biblioteka imbalanced-learn opiera się o metodę `.fit_resample()`, która zwraca zmodyfikowany zbiór uczący (z usuniętymi/dodatkowymi próbkami). Implementuje także zmodyfikowany `Pipeline`, bo ten domyślny ze Scikit-learn nie wspierałby takiej metody. Warto pamiętać o tym, żeby tworzyć nowe zmienne dla zmodyfikowanych zbiorów, bo inaczej trzeba by wykonywać duże części notebooka na nowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_rf_performance(estimator: RandomForestClassifier, X_test, y_test) -> None:\n",
    "    y_score = estimator.predict_proba(X_test)[:, 1]\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    auroc = roc_auc_score(y_test, y_score)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"AUROC: {100 * auroc:.2f}%\")\n",
    "    print(f\"F1-score: {100 * f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "\n",
    "\n",
    "enn = EditedNearestNeighbours()\n",
    "print(f\"Samples before ENN: {len(X_train)}\")\n",
    "X_train_enn, y_train_enn = enn.fit_resample(X_train, y_train)\n",
    "print(f\"Samples after ENN: {len(X_train_enn)}\")\n",
    "\n",
    "clf_rf_csl = RandomForestClassifier(class_weight=\"balanced\", random_state=0, n_jobs=-1)\n",
    "clf_rf_csl.fit(X_train_enn, y_train_enn)\n",
    "\n",
    "assess_rf_performance(clf_rf_csl, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wcześniej AUROC wynosiło 89.30%, a F1-score 28.00%. Mamy spadek obu metryk - niedobrze! Usunęliśmy jednak około 10% zbioru, może to za dużo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 3 (1.5 punktu)**\n",
    "\n",
    "1. Dokonaj tuningu hiperparametrów ENN:\n",
    "   - stwórz siatkę hiperparametrów: \n",
    "     - liczba sąsiadów: `[1, 3, 5]`\n",
    "     - tryb wyboru punktów: `[\"all\", \"mode\"]`\n",
    "   - przed użyciem `GridSearchCV` stwórz pipeline (ten z biblioteki imbalanced-learn), łączący ENN i Random Forest\n",
    "   - wybierz klasyfikator o najwyższym AUROC\n",
    "   - wykorzystaj 10-fold CV - przy zbiorach niezbalansowanych często daje to dokładniejsze oszacowanie\n",
    "   - pamiętaj, żeby podać, którego elementu pipeline'u dotyczą hiperparametry w siatce (np. `enn__n_neighbors`)\n",
    "2. Wypisz znalezione optymalne wartości hiperparametrów. Sprawdź wyniki na zbiorze testowym.\n",
    "3. Czy usuwamy punkty agresywniej, czy bardziej konwerwatywnie? Zweryfikuj swoją intuicję, sprawdzając liczność zbioru przed i po zastosowaniu ENN z optymalnymi hiperparametrami.\n",
    "4. Czy undersampling ostatecznie poprawił wynik? Czy twoim zdaniem warto tu zastosować taką technikę?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Być może oversampling da nam większe korzyści, w końcu klasy pozytywnej jest naprawdę mało. Wypróbujmy najpierw SMOTE z domyślnymi hiperparametrami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "print(f\"Samples before SMOTE: {len(X_train)}\")\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Samples after SMOTE: {len(X_train_smote)}\")\n",
    "\n",
    "clf_rf_csl = RandomForestClassifier(class_weight=\"balanced\", random_state=0, n_jobs=-1)\n",
    "clf_rf_csl.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "assess_rf_performance(clf_rf_csl, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest definitywnie lepiej! Liczba przykładów z klasy pozytywnej wzrosła bardzo mocno, ale dzięki skalowalności lasu losowego nie jest to drastycznie odczuwalne. Za to F1-score bardzo wzrósł, bo zwiększyliśmy znacząco wagę klasy mniejszościowej, i to zagęszczając ją w przestrzeni zbioru danych. Dzięki temu i FP, i FN spadną.\n",
    "\n",
    "Imbalanced-learn domyślnie generuje tyle klasy mniejszościowej, żeby było jej tyle samo, co dominującej. Prawie zawsze powoduje to overfitting - zweryfikujmy to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train metrics\")\n",
    "assess_rf_performance(clf_rf_csl, X_train, y_train)\n",
    "print()\n",
    "print(\"Test metrics\")\n",
    "assess_rf_performance(clf_rf_csl, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to wręcz tragiczny overfitting! Definitywnie trzeba tutaj tuningu. Imbalanced-learn pozwala na to poprzez parametr `sampling_strategy`. Jeżeli jest to liczba, to oznacza stosunek liczby przykładów klasy mniejszościowej do liczby przykładów klasy większościowej po oversamplingu.\n",
    "\n",
    "Przykładowo, domyślne ustawienia odpowiadają `sampling_strategy=1`, czyli:\n",
    "\n",
    "$$\\large\n",
    "\\frac{n_{minority}}{n_{majority}} = 1 \\longrightarrow n_{minority} = n_{majority}\n",
    "$$\n",
    "\n",
    "Żeby zmniejszyć overfitting, trzeba generować mniej klasy pozytywnej, czyli zmniejszyć tę proporcję. Dodatkowo możemy zmienić wartość najbliższych sąsiadów - mniejsza liczba będzie skutkować generacją bardziej wiernych lokalnie próbek, a większa zwiększy różnorodność."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 4 (2 punkty)**\n",
    "\n",
    "Ze względu na koszt obliczeniowy połączenia 10-fold CV i metod opartych o sąsiedztwo można wykonać **step-wise tuning**, w którym robimy walidację skrośną po kolei dla parametrów, zamiast sprawdzać wszystkie kombinacje po kolei. Nie daje to gwarancji optymalności, ale typowo działa bardzo dobrze, a przy tym jest dużo szybsze. Jest to typowo stosowane w boostingu, który ma bardzo dużo hiperparametrów, ale także przy innych kosztownych algorytmach. Dobrze opisuje to [ten artykuł](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258).\n",
    "\n",
    "Dokonaj po kolei tuningu:\n",
    "- liczby sąsiadów w SMOTE w zakresie `[1, 2, 3, 4, 5]`\n",
    "- ilości klasy pozytywnej w zakresie od 0.25 do 1 z krokiem 0.25 (może się przydać `np.linspace()` albo  `np.arange()`)\n",
    "\n",
    "Zwróć uwagę na:\n",
    "- 10-fold CV\n",
    "- ustawienie `random_state=0`\n",
    "- przyda się ustawić `verbose=4`, żeby mieć logi z wykonania, bo będzie się to chwilę liczyć\n",
    "\n",
    "Sprawdź wyniki obu pipeline'ów (z osobna) na zbiorze treningowym oraz testowym. Wytrenuj także łączny pipeline, wykorzystując oba znalezione parametry naraz, i sprawdź jego wyniki.\n",
    "\n",
    "Pamiętaj, że nie trzeba przetrenowywać klasyfikatorów na finalnych hiperparametrach - obiekt `GridSearchCV` też ma metodę `.predict()`, w któryj pod spodem użyje modelu z najlepszymi znalezionymi wartościami hiperparametrów.\n",
    "\n",
    "Skomentuj:\n",
    "- czy wynik się poprawił?\n",
    "- czy zmniejszono lub wyeliminowano overfitting w którymś przypadku?\n",
    "- czy warto było tune'ować oba parametry?\n",
    "- czy połączenie parametrów poprawiło wynik?\n",
    "\n",
    "Oszacuj, ile wolniej wykonywałby się grid search na pełnej, kwadratowej siatce hiperparametrów. Oblicz liczbę modeli, którą trzeba by wytrenować w obu przypadkach (step-wise oraz na pełnej siatce) przy 10-fold CV, i przyjmij stały średni czas na jeden fold według logów z treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatnią rzeczą, którą możemy tu zrobić, jest połączenie naszych technik. Imbalanced-learn implementuje wygodne połączenie oversamplingu z undersamplingu w module `combine`, np. klasą `SMOTEENN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "smote_enn_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"smoteenn\", SMOTEENN(random_state=0)),\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(class_weight=\"balanced\", random_state=0, n_jobs=-1),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "smote_enn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "assess_rf_performance(smote_enn_pipeline, X_train, y_train)\n",
    "assess_rf_performance(smote_enn_pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy domyślnych hiperparametrach, połączenie SMOTE i ENN daje gorsze wyniki niż sam SMOTE. Może jednak to kwestia tuningu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 5 (0.5 punktu)**\n",
    "\n",
    "Wytrenuj SMOTEENN, wykorzystując optymalne hiperparametry znalezione podczas tuningu ENN oraz SMOTE. Sprawdź wyniki na zbiorze testowym.\n",
    "\n",
    "Porównaj wyniki ENN, SMOTE oraz ich połączenia. Które rozwiązanie wybrałbyś w praktyce i dlaczego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja ekstremalnie niezbalansowana i anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako nasz drugi zbiór wykorzystamy [Credit Card Fraud Detection dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). Został on stworzony przez naukowców z Université Libre de Bruxelles we współpracy z firmą Wordline, obsługującą transakcje finansowe. Jest to największa europejska firma tego typu, i jedna z największych na świecie. Na potrzeby tego datasetu udostępniła transakcje z Europy z września 2013 roku.\n",
    "\n",
    "Jest to ponad 284 tysiące transakcji, z czego zaledwie 492 to transakcje będące wynikiem przestępstwa (fraud transaction). Klasa pozytywna to zatem około 0.172% danych, co wymaga specjalnych algorytmów i metryk. Cechy w zbiorze zostały zanonimizowane za pomocą transformacji PCA, dzięki czemu można było publicznie udostępnić taki zbiór. Jedynie publicznie znane cechy to \"Time\" i \"Amount\". Wszystkie cechy są numeryczne i nie ma wartości brakujących, a dane są najwyższej możliwej jakości (generowane automatycznie, a fraud jest bardzo dokładnie sprawdzany jako przestępstwa), więc jest doskonały do uczenia maszynowego.\n",
    "\n",
    "Warto pamiętać, że chociaż fraud to tak mało danych, to każdy jeden przypadek to bardzo ciężkie przestępstwo, często mogące zrujnować komuś życie, więc wykrycie możliwie jak największej liczby z nich obowiązkiem prawnym firm finansowych. Z tego względu algorytmy stanowią tutaj część systemu, flagujące transakcje jako podejrzane według prawdopodobieństwa. Później następuje weryfikacja ręczna w takich wypadkach.\n",
    "\n",
    "Ze względu na powyższe cechy zbioru, autorzy proponują metrykę **Area Under Precision-Recall Curve (AUPRC)**. Trzeba pamiętać, żeby uważać przy łączeniu jej z under- i oversamplingiem, bo zmieniają one proporcję klasy pozytywnej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze względu na bardzo duży rozmiar zbioru najpierw go zmniejszymy, żeby wszystko liczyło się w rozsądnym czasie. Naruszymy przy tym balans klas i zwiększymy stosunek outlierów, ale ze względów czysto praktycznych jesteśmy do tego zmuszeni. Dokonamy takiego losowego undersamplingu, żeby zostało 50 tysięcy próbek z klasy negatywnej i wszystkie z klasy pozytywnej.\n",
    "\n",
    "W praktyce też tak się czasem robi - na nic nam potężna ilość danych, jeżeli nie jesteśmy w stanie nic na tym policzyć. Ostatecznie fraud transaction stanowią dalej niecały 1% naszych danych, więc zbiór dalej jest ekstremalnie niezbalansowany i przybliżenie prawdziwych danych jest dobre.\n",
    "\n",
    "Ma to też tę zaletę, że zwalcza zjawisko nazywane **swamping**. Występuje ono w anomaly detection, gdy mamy totalnie za dużo klasy dominującej i nachodzi ona na chmurę punktów z klasy mniejszościowej (anomalii), \"zalewając\" ją. Powoduje to często FP, kiedy te przykłady z klasy dominującej zostają uznane za pozytywne.\n",
    "\n",
    "Standaryzujemy też dane, bo skorzystamy z metod opartych o najbliższych sąsiadów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"credit_card_fraud_data.parquet\")\n",
    "df = df.drop(columns=\"Time\")\n",
    "y = df.pop(\"Class\")\n",
    "\n",
    "sampling_strategy = {0: 50000, 1: (y == 1).sum()}\n",
    "\n",
    "random_under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "df, y = random_under_sampler.fit_resample(df, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_count = (y == 1).sum()\n",
    "y_pos_perc = y_pos_count / len(y)\n",
    "\n",
    "print(f\"Fraud class after resampling: {100 * y_pos_perc:.2f}% of the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjemy po kolei trzech algorytmów nienadzorowanego outlier detection:\n",
    "- kNN\n",
    "- Local Outlier Factor (LOF)\n",
    "- Isolation Forest\n",
    "\n",
    "Jako wartość parametru `contamination`, czyli oczekiwanej proporcji outlierów, warto zacząć po prostu od ułamka anomalii w zbiorze treningowym, jeżeli jest ona znana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "def assess_anomaly_detection_model(estimator, X_test, y_test) -> None:\n",
    "    y_pred_score = estimator.predict_proba(X_test)\n",
    "\n",
    "    # in PyOD, .predict_proba() sometimes returns probability distribution,\n",
    "    # and sometimes it returns only probability of being anomaly\n",
    "    if len(y_pred_score.shape) > 1:\n",
    "        y_pred_score = y_pred_score[:, 1]\n",
    "\n",
    "    auprc = average_precision_score(y_test, y_pred_score)\n",
    "    print(f\"AUPRC: {100 * auprc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "\n",
    "contamination = (y == 1).sum() / len(y)\n",
    "\n",
    "knn = KNN(contamination=contamination, n_jobs=-1)\n",
    "knn.fit(X_train)\n",
    "print(\"kNN metrics\")\n",
    "assess_anomaly_detection_model(knn, X_test, y_test)\n",
    "print()\n",
    "\n",
    "lof = LOF(contamination=contamination, n_jobs=-1)\n",
    "lof.fit(X_train)\n",
    "print(\"Local Outlier Factor metrics\")\n",
    "assess_anomaly_detection_model(lof, X_test, y_test)\n",
    "print()\n",
    "\n",
    "iforest = IForest(contamination=contamination, random_state=0, n_jobs=-1)\n",
    "iforest.fit(X_train)\n",
    "print(\"Isolation Forest metrics\")\n",
    "assess_anomaly_detection_model(iforest, X_test, y_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pamiętając, że klasy pozytywnej mamy zaledwie 1%, to wynik kNN jest przyzwoity, Isolation Forest dobry, ale Local Outlier Factor całkowicie nie działa. Ale i tak było warto zrobić taki przegląd - pamiętając silne i słabe strony tych algorytmów, możemy wywnioskować, z jakimi anomaliami mamy tu do czynienia.\n",
    "\n",
    "**Zadanie 6 (0.5 punktu)**\n",
    "\n",
    "Mamy 4 podstawowe rodzaje anomalii:\n",
    "- local\n",
    "- global\n",
    "- dependency\n",
    "- clustered\n",
    "\n",
    "Na podstawie powyższych metryk i slajdów o zaletach i wadach algorytmów z zajęć, z jakim typem anomalii mamy tu najprawdopodobniej do czynienia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy takich wynikach nie warto dalej rozważać Local Outlier Factor. kNN wykazuje na pewno potencjał, ale nasz zbiór jest dość duży, więc czuć wolniejsze tempo tej metody, a niestety PyOD nie współgra dobrze z PyNNDescent, żeby go przyspieszyć z użyciem ANN. Dlatego skupimy się teraz na Isolation Forest.\n",
    "\n",
    "Jego najważniejsze hiperparametry to:\n",
    "- `n_estimators` - liczba drzew, typowo ok. 500 jest już osiągana asymptota wyniku\n",
    "- `max_samples` - wielkość próbki per drzewo, domyślnie 256, ale nieco większa może pomóc, jeżeli mamy naprawdę masywny zbiór\n",
    "\n",
    "Typowo `contamination` niewiele zmienia w przypadku tego algorytmu, kiedy używamy metryki opartej o prawdopodobieństwa, takiej jak AUPRC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 7 (1.5 punktu)**\n",
    "\n",
    "1. Dokonaj tuningu hiperarametrów po kolei (step-wise) za pomocą walidacji skrośnej:\n",
    "   - najpierw `n_estimators`, wartości `[100, 200, 300, 400, 500]`\n",
    "   - później `max_samples`,  wartości `[100, 200, 256, 300, 400, 500]`\n",
    "   - wykorzystaj wartość `contamination` obliczoną wcześniej\n",
    "   - użyj `random_state=0` i `n_jobs=-1` dla obiektu `IForest`\n",
    "   - użyj 5-krotnej walidacji skrośnej, optymalizując `\"average_precision\"` (AUPRC)\n",
    "2. Wypisz znalezione optymalne wartości parametrów.\n",
    "3. Wytrenuj Isolation Forest z wartościami obu parametrów. Sprawdź wynik na zbiorze testowym.\n",
    "4. Skomentuj, czy udało się poprawić wynik. Czy twoim zdaniem było warto dokonać tuningu obu hiperparamametrów, czy wystarczyłby jeden z nich?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaprezentowane podejścia należały do **uczenia nienadzorowanego (unsupervised learning)**, gdyż te algorytmy nie potrzebowały klas dla przykładów ze zbioru treningowego. W szczególności Isolation Forest potrafi działać bardzo dobrze nawet wtedy, kiedy zbiór uczący nie zawiera żadnych anomalii. Wykorzystanie takich algorytmów jest zatem proste i tanie, a w szczególności można dla nich łatwo stworzyć potężne zbiory danych.\n",
    "\n",
    "Jeżeli mamy luksus posiadania pełnej informacji o klasach, możemy użyć algorytmów uczenia nadzorowanego (supervised learning). W szczególności można także połączyć te podejścia, co realizuje **uczenie pół-nadzorowane (semi-supervised learning)**, którego przedstawicielem jest XGBoost Outlier Detection (XGBOD). Polega on na obliczeniu anomaly scores dla próbek za pomocą algorytmów nienadzorowanych (np. kNN czy Isolation Forest) i doklejeniu ich jako dodatkowych cech do naszego zbioru treningowego. Można stosować jeden algorytm wielokrotnie, np. kNN dla wielu wartości k, bo wtedy XGBoost ma wiele nowych cech (dla różnych gęstości outlierów) i może je elastycznie łączyć.\n",
    "\n",
    "Tak naprawdę podejście to jest bardzo ogólne, i można by zastosować dowolne połączenia ekstrekcji dodatkowych cech anomalii i klasyfikatorów. XGBOD to po prostu pierwszy zaproponowany przykład takiego algorytmu i działa naprawdę dobrze.\n",
    "\n",
    "PyOD implementuje to w klasie `XGBOD`, która przyjmuje argument `estimator_list`. Jest to lista obiektów klas do nienadzorowanego outlier detection, np. `KNN` czy `IForest` (samych klas, przed treningiem przez `.fit()`). Sam trening i predykcja działa tak jak w przypadku poprzednich algorytmów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 8 (1 punkt)**\n",
    "\n",
    "1. Stwórz listę `estimator_list`, składającą się z:\n",
    "   - algorytmów `KNN` z `n_neighbors`: `[1, 3, 5, 10, 20, 30, 40, 50]`\n",
    "   - algorytmów `IForest` z `n_estimators`: `[50, 100, 200, 300]`\n",
    "   - pamiętaj o przekazaniu `n_jobs=-1` oraz `random_state=0` (w razie potrzeby) podczas tworzenia obiektów tych klas\n",
    "2. Wytrenuj algorytm `XGBOD`, pamiętaj o przekazaniu stworzonego `estimator_list` raz o ustawieniu `n_jobs=-1` i `random_state=0`.\n",
    "3. Dokonaj ewaluacji wyników na zbiorze testowym.\n",
    "4. Skomentuj:\n",
    "   - czy udało się poprawić wynik?\n",
    "   - czy twoim zdaniem taka wartość metryki jest zadowalająca?\n",
    "   - czy trening subiektywnie trwał zauważalnie dłużej od tego dla algorytmów nienadzorowanych?\n",
    "   - czy twoim zdaniem warto ponieść wysiłek i koszty, pozwalające na użycie takiego algorytmu pół-nadzorowanego?\n",
    "\n",
    "**Uwaga:** może się to liczyć dość długo, rzędu kilku minut. Jeżeli będzie definitywnie zbyt długie, zmniejsz liczbę algorytmów KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dodatkowe (3 punkty)\n",
    "\n",
    "W przypadku niektórych zbiorów danych anomalie mogą być zjawiskiem dość pozytywnym, tylko po prostu ekstremalnie rzadkim. Jest tak typowo w farmacji, gdzie molekuły będące potencjalnymi lekami są bardzo niewielkim ułamkiem zbiorów nawet wśród wstępnie typowanych, obiecujących substancji. Pierwszy etap projektowania nowych leków, tzw. high-throughput screening (HTS), polega na identyfikacji tego bardzo niewielkiego podzbioru spośród wielkich baz molekuł, w celu dalszego badania.\n",
    "\n",
    "Zbiór AID746, [dostępny na platformie Kaggle](https://www.kaggle.com/datasets/uciml/bioassay-datasets), dotyczy identyfikacji kinaz białkowych aktywowanych mitogenami ([Wikipedia](https://pl.wikipedia.org/wiki/Kinazy_aktywowane_mitogenami)). Są to enzymy regulujące odpowiedzi na sygnały docierające do komórki, regulujące wiele ciekawych funkcji. Mają potencjalne zastosowania m.in. w rozwoju metod chemoterapii, badaniu insulinoodporności czy rozwoju leków przeciwzapalnych ([Wikipedia](https://en.wikipedia.org/wiki/Mitogen-activated_protein_kinase#As_therapeutic_targets)).\n",
    "\n",
    "W tym zbiorze danych klasa substancji aktywnych stanowi 0.61% zbioru, spośród ok. 57 tysięcy substancji w zbiorze. Jest on już podzielony na część treningową i testową.\n",
    "\n",
    "Dokonaj klasyfikacji oraz tuningu hiperparametrów dla tego zbioru z pomocą:\n",
    "- kNN\n",
    "- LOF\n",
    "- Isolation Forest\n",
    "\n",
    "Wytrenuj także model XGBOD, wykorzystując grupy najbardziej obiecujących modeli.\n",
    "\n",
    "Jako metryki użyj AUPRC. Podaj także czułość (recall) finalnego algorytmu - w końcu na etapie początkowego filtrowania substancji chcemy na pewno mieć jak najmniej false negatives.\n",
    "\n",
    "Na podstawie wyników oceń, z jakim typem anomalii mamy tu do czynienia. Czy udało się uzyskać zadowalające w twojej ocenie wyniki?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUM",
   "language": "python",
   "name": "pum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
