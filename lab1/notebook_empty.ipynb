{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele liniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjemy standardowego stosu narzędzi do tabelarycznego uczenia maszynowego w Pythonie: Numpy, Pandas, Matplotlib i Scikit-learn. Użyjemy też missingno dla łatwej wizualizacji wartości brakujących."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy pandas matplotlib scikit-learn missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbiór danych do regresji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystamy zbiór danych [Ames housing](https://www.openintro.org/book/statdata/?data=ames), w którym zadaniem jest przewidywanie wartości domu na podstawie cech budynku, działki, lokalizacji itp. Jest to więc przewidywanie wartości ciągłej, czyli regresja. Obejmuje zmienne do usunięcia (np. ID transakcji), numeryczne (floaty i inty), kategoryczne nieuporządkowane (*categorical nominal*) oraz kategoryczne uporządkowane (*categorical ordinal*), więc będzie wymagał wstępnego przetworzenia tak jak większość prawdziwych danych w ML.\n",
    "\n",
    "Inne znane, ale gorsze jakościowo zbiory tego typu to na przykład:\n",
    "- Boston housing - rasistowski, z tego powodu usunięty np. ze Scikit-learn ([wyjaśnienie](https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html), [dyskusja](https://github.com/quantumblacklabs/causalnex/issues/92), [badanie](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8))\n",
    "- California housing - zbyt prosty (tylko kilka zmiennych numerycznych), użyty np. w książce \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" A. Geron ([opis](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html))\n",
    "\n",
    "Autor zbioru to Dean De Cock, a zbiór został opisany oryginalnie w [tym artykule](https://jse.amstat.org/v19n3/decock.pdf). Szczegółowe opisy zmiennych znajdują się w pliku `ames_description.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ames_data.csv\")\n",
    "\n",
    "# remove dots from names to match data_description.txt\n",
    "df.columns = [col.replace(\".\", \"\") for col in df.columns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksploracja danych, czyszczenie danych i inżynieria cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wstępne czyszczenie danych (*data cleaning*) obejmuje:\n",
    "- usunięcie kolumn `Order` oraz `PID`\n",
    "- usunięcie obserwacji odstających (*outliers*), które mają powyżej 4000 stóp kwadratowych (ok. 370 metrów kwadratowych) powierzchni\n",
    "- usunięcie wpisów z dzielnic `GrnHill` oraz `Landmrk`, które obejmują w sumie zaledwie 3 domy\n",
    "- transformacja logarytmiczna zmiennej zależnej (ceny domu)\n",
    "\n",
    "To drugie jest motywowane wykresem przedstawionym poniżej. Zostało uznane za błąd już przez samego autora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Order\", \"PID\"], axis=\"columns\")\n",
    "df = df.loc[~df[\"Neighborhood\"].isin([\"GrnHill\", \"Landmrk\"]), :]\n",
    "\n",
    "plt.scatter(df[\"GrLivArea\"], df[\"SalePrice\"])\n",
    "plt.title(\"House area vs price\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()\n",
    "\n",
    "df = df.loc[df[\"GrLivArea\"] <= 4000, :]\n",
    "\n",
    "plt.scatter(df[\"GrLivArea\"], df[\"SalePrice\"])\n",
    "plt.title(\"House area vs price, outliers removed\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"GrLivArea\"] <= 4000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zawsze warto też przyjrzeć się rozkładowi zmiennej docelowej, żeby poznać jego typ i skalę. Jak widać poniżej, rozkład jest dość skośny, co ma sens - mało jest bardzo drogich domów.\n",
    "\n",
    "Rozkład normalny jest zwykle korzystniejszy dla tworzenia modeli, bo daje sensowną \"wartość środkową\" do przewidywania, a także penalizuje tak samo mylenie się w obie strony (zaniżona i zawyżona predykcja). Dokonamy dlatego **transformacji logarytmicznej (log transform)**, czyli zlogarytmujemy zmienną docelową (zależną). Dla stabilności numerycznej używa się zwykle `np.log1p`, a nie `np.log`.\n",
    "\n",
    "Dodatkowa korzyść z takiej transformacji jest taka, że regresja liniowa przewiduje dowolne wartości rzeczywiste. Po przekształceniu logarytmicznym jest to całkowicie ok, natomiast w oryginalnej przestrzeni trzeba by wymusić przewidywanie tylko wartości pozytywnych (negatywne ceny są bez sensu). Da się to zrobić, ale zwiększa to koszt obliczeniowy. Operowanie na tzw. log-price jest bardzo częste w finansach.\n",
    "\n",
    "**Uwaga:** - upewnij się, żeby nie wykonać komórki z kodem logarytmowania więcej niż raz, bo wtedy zlogarytmujesz wartości docelowe ponownie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SalePrice\"].hist()\n",
    "plt.title(\"Original sale price\")\n",
    "plt.show()\n",
    "\n",
    "df.loc[:, \"SalePrice\"] = np.log1p(df.loc[:, \"SalePrice\"])\n",
    "\n",
    "pd.Series(np.log(df[\"SalePrice\"])).hist()\n",
    "plt.title(\"Log sale price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ramach **wstępnej eksploracji danych (Exploratory Data Analysis, EDA)** sprawdźmy też wartości brakujące. Są zmienne, które mają poniżej 10% wartości, a bardzo znacząca liczba cech ma co najmniej 10% braków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ramach dalszego czyszczenia danych uzupełnimy wartości brakujące. Trzeba tu jednak wziąć pod uwagę:\n",
    "- mamy dużo zmiennych kategorycznych - nie można w nich dokonać imputacji modą, medianą itp.\n",
    "- mamy często dużo wartości brakujących - estymacja modą byłaby niedokładna\n",
    "- mamy dużą wiedzę o zmiennych dzięki plikowi z opisami\n",
    "\n",
    "Można więc zastosować odpowiednią wiedzę i przyjąć wartości domyślne. Przykładowo, brak informacji o powierzchni piwnicy możemy uznać po prostu za brak piwnicy, i wpisać tam odpowiednią wartość. W przypadku niektórych zmiennych może doprowadzić to do stworzenia nowej wartości, która implicite będzie reprezentować wartość brakującą.\n",
    "\n",
    "Znaczna część poniższej analizy została zainspirowana [tym notebookiem na Kaggle](https://www.kaggle.com/code/juliencs/a-study-on-regression-applied-to-the-ames-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_na(df: pd.DataFrame, col: str, value) -> None:\n",
    "    df.loc[:, col] = df.loc[:, col].fillna(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alley : data description says NA means \"no alley access\"\n",
    "replace_na(df, \"Alley\", value=\"None\")\n",
    "\n",
    "# BedroomAbvGr : NA most likely means 0\n",
    "replace_na(df, \"BedroomAbvGr\", value=0)\n",
    "\n",
    "# BsmtQual etc : data description says NA for basement features is \"no basement\"\n",
    "replace_na(df, \"BsmtQual\", value=\"No\")\n",
    "replace_na(df, \"BsmtCond\", value=\"No\")\n",
    "replace_na(df, \"BsmtExposure\", value=\"No\")\n",
    "replace_na(df, \"BsmtFinType1\", value=\"No\")\n",
    "replace_na(df, \"BsmtFinType2\", value=\"No\")\n",
    "replace_na(df, \"BsmtFullBath\", value=0)\n",
    "replace_na(df, \"BsmtHalfBath\", value=0)\n",
    "replace_na(df, \"BsmtUnfSF\", value=0)\n",
    "\n",
    "# Condition : NA most likely means Normal\n",
    "replace_na(df, \"Condition1\", value=\"Norm\")\n",
    "replace_na(df, \"Condition2\", value=\"Norm\")\n",
    "\n",
    "# External stuff : NA most likely means average\n",
    "replace_na(df, \"ExterCond\", value=\"TA\")\n",
    "replace_na(df, \"ExterQual\", value=\"TA\")\n",
    "\n",
    "# Fence : data description says NA means \"no fence\"\n",
    "replace_na(df, \"Fence\", value=\"No\")\n",
    "\n",
    "# Functional : data description says NA means typical\n",
    "replace_na(df, \"Functional\", value=\"Typ\")\n",
    "\n",
    "# GarageType etc : data description says NA for garage features is \"no garage\"\n",
    "replace_na(df, \"GarageType\", value=\"No\")\n",
    "replace_na(df, \"GarageFinish\", value=\"No\")\n",
    "replace_na(df, \"GarageQual\", value=\"No\")\n",
    "replace_na(df, \"GarageCond\", value=\"No\")\n",
    "replace_na(df, \"GarageArea\", value=0)\n",
    "replace_na(df, \"GarageCars\", value=0)\n",
    "\n",
    "# HalfBath : NA most likely means no half baths above grade\n",
    "replace_na(df, \"HalfBath\", value=0)\n",
    "\n",
    "# HeatingQC : NA most likely means typical\n",
    "replace_na(df, \"HeatingQC\", value=\"Ta\")\n",
    "\n",
    "# KitchenAbvGr : NA most likely means 0\n",
    "replace_na(df, \"KitchenAbvGr\", value=0)\n",
    "\n",
    "# KitchenQual : NA most likely means typical\n",
    "replace_na(df, \"KitchenQual\", value=\"TA\")\n",
    "\n",
    "# LotFrontage : NA most likely means no lot frontage\n",
    "replace_na(df, \"LotFrontage\", value=0)\n",
    "\n",
    "# LotShape : NA most likely means regular\n",
    "replace_na(df, \"LotShape\", value=\"Reg\")\n",
    "\n",
    "# MasVnrType : NA most likely means no veneer\n",
    "replace_na(df, \"MasVnrType\", value=\"None\")\n",
    "replace_na(df, \"MasVnrArea\", value=0)\n",
    "\n",
    "# MiscFeature : data description says NA means \"no misc feature\"\n",
    "replace_na(df, \"MiscFeature\", value=\"No\")\n",
    "replace_na(df, \"MiscVal\", value=0)\n",
    "\n",
    "# OpenPorchSF : NA most likely means no open porch\n",
    "replace_na(df, \"OpenPorchSF\", value=0)\n",
    "\n",
    "# PavedDrive : NA most likely means not paved\n",
    "replace_na(df, \"PavedDrive\", value=\"N\")\n",
    "\n",
    "# PoolQC : data description says NA means \"no pool\"\n",
    "replace_na(df, \"PoolQC\", value=\"No\")\n",
    "replace_na(df, \"PoolArea\", value=0)\n",
    "\n",
    "# ScreenPorch : NA most likely means no screen porch\n",
    "replace_na(df, \"ScreenPorch\", value=0)\n",
    "\n",
    "# TotRmsAbvGrd : NA most likely means 0\n",
    "replace_na(df, \"TotRmsAbvGrd\", value=0)\n",
    "\n",
    "# Utilities : NA most likely means all public utilities\n",
    "replace_na(df, \"Utilities\", value=\"AllPub\")\n",
    "\n",
    "# WoodDeckSF : NA most likely means no wood deck\n",
    "replace_na(df, \"WoodDeckSF\", value=0)\n",
    "\n",
    "# FireplaceQu : data description says NA means \"no fireplace\"\n",
    "replace_na(df, \"FireplaceQu\", value=\"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 1 (0.5 punktu)**\n",
    "\n",
    "Z pomocą dokumentacji zmiennych w pliku `data_description.txt` zdecyduj, jakie wartości domyślne przypisać zmiennym:\n",
    "- `CentralAir`\n",
    "- `EnclosedPorch`\n",
    "- `Fireplaces`\n",
    "- `SaleCondition`\n",
    "\n",
    "W praktyce niestety zwykle nie jest tak różowo, że mamy dokumentację, i ten krok zajmuje kilka godzin (lub dni) u różnych osób w firmie :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszej kolejności zamienimy zmienne `MSSubClass` i `MoSold` z numerycznych na kategoryczne, zgodnie z ich znaczeniem. Zakodujemy także zmienne kategoryczne uporządkowane (categorical ordinal) z tekstowych na kolejne liczby całkowite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.replace(\n",
    "    {\n",
    "        \"MSSubClass\": {\n",
    "            20: \"SC20\",\n",
    "            30: \"SC30\",\n",
    "            40: \"SC40\",\n",
    "            45: \"SC45\",\n",
    "            50: \"SC50\",\n",
    "            60: \"SC60\",\n",
    "            70: \"SC70\",\n",
    "            75: \"SC75\",\n",
    "            80: \"SC80\",\n",
    "            85: \"SC85\",\n",
    "            90: \"SC90\",\n",
    "            120: \"SC120\",\n",
    "            150: \"SC150\",\n",
    "            160: \"SC160\",\n",
    "            180: \"SC180\",\n",
    "            190: \"SC190\",\n",
    "        },\n",
    "        \"MoSold\": {\n",
    "            1: \"Jan\",\n",
    "            2: \"Feb\",\n",
    "            3: \"Mar\",\n",
    "            4: \"Apr\",\n",
    "            5: \"May\",\n",
    "            6: \"Jun\",\n",
    "            7: \"Jul\",\n",
    "            8: \"Aug\",\n",
    "            9: \"Sep\",\n",
    "            10: \"Oct\",\n",
    "            11: \"Nov\",\n",
    "            12: \"Dec\",\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(\n",
    "    {\n",
    "        \"Alley\": {\"None\": 0, \"Grvl\": 1, \"Pave\": 2},\n",
    "        \"BsmtCond\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"BsmtExposure\": {\"No\": 0, \"Mn\": 1, \"Av\": 2, \"Gd\": 3},\n",
    "        \"BsmtFinType1\": {\n",
    "            \"No\": 0,\n",
    "            \"Unf\": 1,\n",
    "            \"LwQ\": 2,\n",
    "            \"Rec\": 3,\n",
    "            \"BLQ\": 4,\n",
    "            \"ALQ\": 5,\n",
    "            \"GLQ\": 6,\n",
    "        },\n",
    "        \"BsmtFinType2\": {\n",
    "            \"No\": 0,\n",
    "            \"Unf\": 1,\n",
    "            \"LwQ\": 2,\n",
    "            \"Rec\": 3,\n",
    "            \"BLQ\": 4,\n",
    "            \"ALQ\": 5,\n",
    "            \"GLQ\": 6,\n",
    "        },\n",
    "        \"BsmtQual\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"ExterCond\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"ExterQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"FireplaceQu\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"Functional\": {\n",
    "            \"Sal\": 1,\n",
    "            \"Sev\": 2,\n",
    "            \"Maj2\": 3,\n",
    "            \"Maj1\": 4,\n",
    "            \"Mod\": 5,\n",
    "            \"Min2\": 6,\n",
    "            \"Min1\": 7,\n",
    "            \"Typ\": 8,\n",
    "        },\n",
    "        \"GarageCond\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"GarageQual\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"HeatingQC\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"KitchenQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"LandSlope\": {\"Sev\": 1, \"Mod\": 2, \"Gtl\": 3},\n",
    "        \"LotShape\": {\"IR3\": 1, \"IR2\": 2, \"IR1\": 3, \"Reg\": 4},\n",
    "        \"PavedDrive\": {\"N\": 0, \"P\": 1, \"Y\": 2},\n",
    "        \"PoolQC\": {\"No\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "        \"Street\": {\"Grvl\": 1, \"Pave\": 2},\n",
    "        \"Utilities\": {\"ELO\": 1, \"NoSeWa\": 2, \"NoSewr\": 3, \"AllPub\": 4},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W Pandasie zmienne kategoryczne, napisy itp. są typu `object`, natomiast numeryczne mają typy z Numpy'a, np. `int64`. Oczywiście oba rodzaje zmiennych trzeba przetwarzać na inne sposoby, więc zapiszemy to od razu. Wyodrębnimy też wektor docelowy.\n",
    "\n",
    "Od razu dokonamy też podziału na zbiór treningowy i testowy. Jako że nasz zbiór jest dość mały, to podział będzie 70%-30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df.pop(\"SalePrice\")\n",
    "\n",
    "categorical_features = df.select_dtypes(include=\"object\").columns\n",
    "numerical_features = df.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz trzeba dokonać transformacji naszych danych:\n",
    "- zmienne kategoryczne trzeba zamienić na osobne kolumny - **one-hot encoding**\n",
    "- zmienne numeryczne dalej mogą mieć wartości brakujące i trzeba je **imputować (impute**)\n",
    "- zmienne numeryczne trzeba przeskalować do zakresu wartości $[0, 1]$ - **normalization / min-max scaling**\n",
    "\n",
    "Mamy zatem 2 operacje do wykonania na zmiennych numerycznych po kolei (imputacja + skalowanie), a równolegle 1 na kategorycznych. Służą do tego w Scikit-learn'ie klasy:\n",
    "- `OneHotEncoder`, `SimpleImputer`, `MinMaxScaler` - transformacje, implementują `.fit()` i `.transform()`\n",
    "- `Pipeline` - do układania transformacji sekwencyjnie\n",
    "- `ColumnTransformer` - do układania transformacji równolegle, dla różnych kolumn\n",
    "\n",
    "**Ważne:** jako że zaraz skorzystamy z regresji liniowej, do klasy `OneHotEncoder` trzeba przekazać `drop=\"first\"`. Stworzy to 1 zmienną mniej, niż typowy one-hot encoding, np. `pd.get_dummies()`, gwarantując brak **idealnie współliniowych zmiennych (perfectly collinear features)**, który byłby niestabilny numerycznie. Dodatkowo, jako że przekształcamy już po podziale na zbiór treningowy i testowy, to możemy spotkać na zbiorze testowym nieliczne przypadki kategorii, których nie ma w zbiorze treningowym - kodujemy je wtedy po prostu jako zera za pomocą `handle_unknown=\"ignore\"`.\n",
    "\n",
    "**Zadanie 2 (0.5 punktu)**\n",
    "\n",
    "Stwórz pipeline'y dla zmiennych kategorycznych i numerycznych. Połącz je następnie z użyciem `ColumnTransformer`. \"Wytrenuj\" go na danych treningowych, a następnie przetransformuj dane treningowe oraz testowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(\n",
    "    drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    ")\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "categorical_pipeline = ...\n",
    "\n",
    "numerical_pipeline = ...\n",
    "\n",
    "column_transformer = ...\n",
    "\n",
    "# fit and transform\n",
    "\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja liniowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy teraz przejść do przewidywania wartości domów. Naszym narzędziem będzie tutaj **regresja liniowa (linear regression)**, czyli model postaci:\n",
    "$$\n",
    "\\hat{y} = w \\cdot x = \\sum_{i=1}^{d} w_i x_i\n",
    "$$\n",
    "gdzie $n$ to liczba punktów w zbiorze, a $d$ to wymiarowość. Zakładamy też dodanie punktu przecięcia ze środkiem układu współrzędnych (*bias / intercept*) jako stałej cechy samych jedynek.\n",
    "\n",
    "Jako że używamy wielu zmiennych, to formaliści nazwaliby ten model wielokrotną regresją liniową (*multiple linear regression*), w odróżnieniu od prostej regresji liniowej (*simple linear regression*), która używa tylko 1 cechy. Takie rozróżnienie jest bardziej typowe dla nauk, które wykorzystują regresję głównie do analizy statystycznej, a nie predykcji, jak np. biologia czy medycyna.\n",
    "\n",
    "Model ten minimalizuje **błąd średniokwadratowy (mean squared error, MSE)**:\n",
    "$$\n",
    "L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y - \\hat{y} \\right)^2\n",
    "$$\n",
    "\n",
    "W Scikit-learn ten model implementuje klasa `LinearRegression`. Ważne cechy:\n",
    "- domyślnie uwzględnia intercept (bias) przez `fit_intercept=True`; jest to niepotrzebne i może powodować problemy numeryczne, kiedy nasze dane są już wycentrowane\n",
    "- używa implementacji z pseudoodwrotnością Moore'a-Pendrose'a (SVD)\n",
    "- nie pozwala na regularyzację, do tego trzeba użyć innych klas\n",
    "\n",
    "Sprawdźmy, jak taki model sobie radzi. Do ewaluacji będziemy używać **Root MSE (RMSE)**, czyli pierwiastka kwadratowego z MSE. Ma ważne zalety:\n",
    "- regresja liniowa z definicji modelu opiera się o MSE, więc używamy metryki dobrze związanej z modelem\n",
    "- ma tę samą jednostkę, co przewidywane wartości, dzięki pierwiastkowaniu\n",
    "Jest też dość czuła na outliery, ale może to być korzystne, w zależności od zastosowania.\n",
    "\n",
    "$$\n",
    "RMSE(y, \\hat{y}) = \\sqrt{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "W Scikit-learn RMSE liczy się dość specyficznie, bo używa się funkcji do MSE z argumentem `squared=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# all variables are in range [0, 1], so we don't need an intercept\n",
    "reg_linear = LinearRegression(fit_intercept=False)\n",
    "reg_linear.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg_linear.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czy taki błąd to duży, czy mały błąd? Wszystko zależy od skali wartości przewidywanych. Trzeba pamiętać, że dokonaliśmy logarytmowania zmiennej docelowej, więc trzeba to sprawdzić po transformacji odwrotnej `np.expm1`. Po tej operacji jednostką będą oryginalne dolary.\n",
    "\n",
    "Dodatkowo trzeba zweryfikować, czy przeuczamy (overfitujemy). Sprawdza się to następująco:\n",
    "- obliczamy błąd treningowy oraz testowy\n",
    "- jeżeli oba błędy są wysokie, to mamy underfitting i trzeba użyć pojemniejszego modelu\n",
    "- jeżeli błąd treningowy jest dużo niższy od testowego, to mamy overfitting i trzeba regularyzować\n",
    "\n",
    "W praktyce paradoksalnie często model o większej pojemności z mocną regularyzacją działa lepiej od prostszego modelu ze słabą regularyzacją."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 3 (0.5 punktu)**\n",
    "\n",
    "Uzupełnij kod funkcji `assess_regression_model`:\n",
    "- obliczenie predykcji treningowych oraz testowych\n",
    "- transformacje eksponencjalne, żeby wrócić do oryginalnej jednostki (dolara)\n",
    "- obliczenie RMSE treningowego i testowego\n",
    "- wypisywanie RMSE, zoakrąglonego do 2 miejsc po przecinku\n",
    "\n",
    "Użyj następnie tej funkcji, aby ocenić, czy następuje overfitting, skomentuj to poniżej. Oceń także, czy subiektywnie to duża wartość, biorąc pod uwagę rozkład zmiennej docelowej (wartości i wykresy w sekcji EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_regression_model(model, X_train, X_test, y_train, y_test) -> None:\n",
    "    # predict for train and test\n",
    "\n",
    "    # exponential transform for y_train, y_test and predictions\n",
    "\n",
    "    # calculate train and test RMSE\n",
    "    \n",
    "    # print train and test RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_regression_model(reg_linear, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja regularyzowana (ridge, LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularyzacja zmniejsza pojemność modelu regresji liniowej, narzucając mniejsze wagi poprzez penalizację dużych wag w funkcji kosztu:\n",
    "$$\n",
    "L_{ridge}(y, \\hat{y}) = \\frac{1}{n} (y - \\hat{y})^2 + \\lambda ||w||_2^2 \\\\\n",
    "L_{LASSO}(y, \\hat{y}) = \\frac{1}{n} (y - \\hat{y})^2 + \\alpha ||w||_1 \\\\\n",
    "L_{ElasticNet}(y, \\hat{y}) = \\frac{1}{n} (y - \\hat{y})^2 + \\lambda ||w||_2^2 + \\alpha ||w||_1 \\\\\n",
    "$$\n",
    "\n",
    "Regresja ridge zmniejsza wagi i jest różniczkowalna (szybsza i łatwiejsza w treningu), natomiast regresja LASSO dokonuje selekcji cech, zmniejszając często wagi cech dokładnie do zera. Oba naraz realizuje model ElasticNet.\n",
    "\n",
    "W Scikit-learn implementują je klasy `Ridge`, `Lasso` oraz `ElasticNet`. Najważniejszy hiperparametr każdego z tych modeli to siła regularyzacji, która we wszystkich klasach to `alpha`. Scikit-learn definiuje regularyzację ElasticNet dość specyficznie, za pomocą parametru `l1_ratio`, który wyznacza, jaki ułamek siły regularyzacji przypada dla L1, a jaki dla L2:\n",
    "$$\n",
    "L_{ElasticNet}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y - \\hat{y} \\right)^2 + \\alpha \\cdot (1 - L1\\_ratio) \\cdot ||w||_2^2 + \\alpha \\cdot L1\\_ratio \\cdot ||w||_1 \\\\\n",
    "$$\n",
    "\n",
    "Inne ważne uwagi:\n",
    "- liczba iteracji `max_iter` wyznacza liczbę iteracji solwera; im więcej, tym dokładniejsze rozwiązanie, ale tym dłuższy czas obliczeń\n",
    "- jeżeli `max_iter` będzie zbyt małe i algorytm nie osiągnie zbieżności, to dostaniemy ostrzeżenie, wtedy zwykle trzeba po prostu ją zwiększyć, np. 10-krotnie\n",
    "- jeżeli nie potrzebujemy bardzo precyzyjnego rozwiązania, można ustawić większe `tol` dla przyspieszenia obliczeń\n",
    "\n",
    "Jako że nasz model jest regularyzowany i nie ma ryzyka problemów numerycznych, to teraz już obliczamy intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "reg_ridge = Ridge(random_state=0)\n",
    "reg_lasso = Lasso(random_state=0)\n",
    "\n",
    "reg_ridge.fit(X_train, y_train)\n",
    "reg_lasso.fit(X_train, y_train)\n",
    "\n",
    "assess_regression_model(reg_ridge, X_train, X_test, y_train, y_test)\n",
    "print()\n",
    "assess_regression_model(reg_lasso, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku regularyzacji L2 domyślna siła regularyzacji (`alpha=1.0`) znacząco poprawiła wynik, natomiast w przypadku L1 mamy silny underfitting. Przyda się zastosować tutaj tuning hiperparametrów.\n",
    "\n",
    "W przypadku regresji liniowej istnieją bardzo wydajne implementacje walidacji skrośnej, głównie dzięki prostocie tego modelu. W Scikit-learn są to odpowiednio `RidgeCV`, `LassoCV` oraz `ElasticNetCV`.\n",
    "\n",
    "`RidgeCV` domyślnie wykorzystuje efektywną implementację Leave-One-Out Cross-Validation (LOOCV), w którym zbiorem walidacyjnym jest w danej chwili jedna próbka. Jest to możliwe dzięki pewnym sztuczkom opartym na algebrze liniowej, wyjaśnionych [w dokumentacji w kodzie](https://github.com/scikit-learn/scikit-learn/blob/8c9c1f27b7e21201cfffb118934999025fd50cca/sklearn/linear_model/_ridge.py#L1547) (dla zainteresowanych). Co ważne, jest to operacja o wiele szybsza niż osobne grid search + ridge regression, a nawet od `RidgeCV` z mniejszą liczbą foldów.\n",
    "\n",
    "`LassoCV` oraz `ElasticNetCV` iterują od najmniejszych do największych wartości `alpha` (siły regularyzacji), używając rozwiązania dla mniejszej siły regularyzacji jako punktu początkowego dla kolejnej wartości. Odpowiada to po prostu dość inteligentnemu wyborowi punktu startowego w optymalizacji funkcji kosztu, a znacznie obniża koszt obliczeniowy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 4 (0.5 punktu)**\n",
    "\n",
    "Użyj klas `RidgeCV` oraz `LassoCV` do tuningu hiperparametrów.\n",
    "\n",
    "Dla `RidgeCV` sprawdź 1000 wartości `[0.1, 100]` w skali liniowej - przyda się `np.linspace()`. Użyj LOOCV.\n",
    "\n",
    "Dla `LassoCV` Scikit-learn sam dobierze wartości, musisz podać tylko liczbę wartości alfa do sprawdzenia - użyj 1000. Użyj 5-fold CV. Pamiętaj o podaniu `random_state=0` - solwer jest niedeterministyczny.\n",
    "\n",
    "Wypisz znalezione optymalne wartości siły regularyzacji `.alpha_` dla obu modeli, zaokrąglone do 4 miejsca po przecinku dla czytelności.\n",
    "\n",
    "Przetestuj modele z użyciem `assess_regression_model()`. Skomentuj wyniki. Czy udało się wyeliminować overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy teraz, jak zmieniły się maksymalne wielkości współczynników (w sensie wartości bezwzględnej) dla cech dla regularyzacji L2, oraz ile cech zostało wyeliminowanych przez regularyzację L1.\n",
    "\n",
    "**Uwaga:** kod zakłada, że zmienne to `reg_ridge_cv` oraz `reg_lasso_cv`. Jeżeli u ciebie są inne, dostosuj poniższy kod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_linear_max = np.max(np.abs(reg_linear.coef_))\n",
    "coeffs_ridge_max = np.max(np.abs(reg_ridge.coef_))\n",
    "coeffs_ridge_cv_max = np.max(np.abs(reg_ridge_cv.coef_))\n",
    "\n",
    "coeffs_linear_zero_perc = np.sum(reg_linear.coef_ == 0) / len(reg_linear.coef_)\n",
    "coeffs_lasso_zero_perc = np.sum(reg_lasso.coef_ == 0) / len(reg_linear.coef_)\n",
    "coeffs_lasso_cv_zero_perc = np.sum(reg_lasso_cv.coef_ == 0) / len(reg_linear.coef_)\n",
    "\n",
    "print(\"Coefficients magnitudes (linear, ridge, ridge with CV)\")\n",
    "print(\n",
    "    f\"Max: {coeffs_linear_max:.2f} vs\",\n",
    "    f\"vs {coeffs_ridge_max:.2f} vs\",\n",
    "    f\"{coeffs_ridge_cv_max:.2f}\",\n",
    ")\n",
    "print()\n",
    "print(\"Percentage of eliminated features (linear, LASSO, LASSO with CV)\")\n",
    "print(\n",
    "    f\"{100 * coeffs_linear_zero_perc:.2f}% vs\",\n",
    "    f\"{100 * coeffs_lasso_zero_perc:.2f}% vs\",\n",
    "    f\"{100 * coeffs_lasso_cv_zero_perc:.2f}%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 5 (0.5 punktu)**\n",
    "\n",
    "Dokonaj interpretacji zmian we współczynnikach dla regresji ridge oraz LASSO. Zauważ też, że LASSO bez tuningu hiperparametrów wyeliminowało wszystkie cechy, a my nie trenujemy punktu przecięcia (intercept). Co oznacza taki model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto zwrócić uwagę, że zbieranie i obliczanie cech jest często drogie w praktyce. W tym przypadku model z mniejszą ilością cech dodatkowo radzi sobie dobrze, więc jest tańszy i prostszy, są to znaczące zalety. Zawsze warto się przyjrzeć temu, co model uznaje za bardzo wartościowe oraz mało wartościowe cechy, bo daje nam to wiedzę, na czym się skupić w przyszłości.\n",
    "\n",
    "Jako że dokonaliśmy wcześniej wielu transformacji, to nazwy kolumn są dość mocno poprzekształcane. Na szczęście wszystkie obiekty transformujące w Scikit-learn (z metodą `.transform()`) implementują też metodę `.get_feature_names_out()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 6 (0.75 punktu)**\n",
    "\n",
    "Wyczyść nazwy cech, usuwając z nich przedrostek `\"num_pipeline__\"` oraz `\"cat_pipeline__\"`.\n",
    "\n",
    "Następnie narysuj poziomy wykres słupkowy 10 najważniejszych cech według regresji LASSO po tuningu. Wartościami mają być współczynniki cech. Użyj nazw cech do opisu wykresu. Zadbaj o odpowiedni tytuł.\n",
    "\n",
    "Wypisz także posortowaną listę nazw wyeliminowanych cech.\n",
    "\n",
    "Dokonaj interpretacji 2 wybranych cech spośród najlepszych oraz 2 wybranych spośród tych wyeliminowanych - przyda się dokumentacja do zbioru danych. Czy ta ocena dokonana przez model ma twoim zdaniem sens?\n",
    "\n",
    "Przydatna może być zamiana z `np.ndarray` na `pd.Series`, argument `key` w metodzie `.sort_values()`, oraz metoda `.plot.barh()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja wielomianowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresja wielomianowa to po prostu dodanie wielomianów cech do naszych danych:\n",
    "$$\n",
    "[a, b, c, d] -> [a, b, c, d, a^2, b^2, c^2, d^2, ab, ac, ad, bc, bd, cd]\n",
    "$$\n",
    "\n",
    "W Scikit-learn regresja wielomianowa składa się z 2 osobnych kroków: wygenerowania cech wielomianowych i użycia zwykłej regresji liniowej. Pozwala to na użycie tej transformacji dla dowolnych algorytmów, nie tylko regresji liniowej.\n",
    "\n",
    "Kwestią sporną jest, czy jest sens przeprowadzać taką transformację dla zmiennych po one-hot encodingu. Potęgi na pewno nie mają sensu, natomiast interakcje realizują po prostu operację koniunkcji (AND), ale łatwo prowadzi to do eksplozji wymiarowości. Dla uproszczenia poniżej zastosujemy transformację dla wszystkich cech.\n",
    "\n",
    "Warto pamiętać, że jeżeli używamy modelu, który sam dodaje intercept (jak regresja liniowa), to trzeba przekazać `include_bias=False`. Żeby wymiarowość nam nie urosła zbyt bardzo, użyjemy `interaction_only=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly_features.fit(X_train)\n",
    "\n",
    "X_train_poly = poly_features.transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "reg_ridge_cv_poly = RidgeCV(alphas=np.linspace(0.1, 100, 1000))\n",
    "reg_ridge_cv_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "assess_regression_model(reg_ridge_cv_poly, X_train_poly, X_test_poly, y_train, y_test)\n",
    "print()\n",
    "print(f\"Ridge + polynomial features alpha: {reg_ridge_cv_poly.alpha_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co ciekawe, model bardziej zbliżył się do przeuczenia, ale błąd testowy zmalał. Jest to niezbyt częste, ale możliwe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja logistyczna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do klasyfikacji wykorzystamy zbiór [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing), w którym przewiduje się, czy dana osoba będzie zainteresowana lokatą terminową w banku. Precyzyjny targetowany marketing jest ważny z perspektywy biznesu, bo w praktyce chce się reklamować tak mało, jak to możliwe. Bank zarabia tylko na tych osobach, które są faktycznie zainteresowane reklamą, a pozostałych można łatwo zrazić zbyt dużą liczbą reklam, więc precyzyjna ocena przynosi tu realne zyski.\n",
    "\n",
    "Zbiór posiada dwie wersje, uproszczoną oraz rozszerzoną o dodatkowe atrybuty socjoekonomiczne (np. sytuację ekonomiczną w planowanym momencie reklamy). Wykorzystamy tę drugą, bo są to bardzo wartościowe cechy. Dodatkowo każda wersja posiada pełen zbiór (ok. 45 tysięcy przykładów) oraz pomniejszony (ok. 4 tysiąca przykładów). Dzięki skalowalności regresji logistycznej możemy bez problemu wykorzystać pełny zbiór z dodatkowymi cechami.\n",
    "\n",
    "Opisy zmiennych znajdują się w pliku `bank_marketing_description.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 7 (1 punkt)**\n",
    "\n",
    "*Wczytywanie i czyszczenie danych*\n",
    "\n",
    "1. Załaduj zbiór danych z pliku `bank_marketing_data.csv` do DataFrame'a. Zwróć uwagę, że separatorem jest średnik (argument `sep`).\n",
    "2. Usuń kolumny:\n",
    "  - `default`, czy klient ma zadłużenie na karcie kredytowej; ma tylko 3 wartości `yes`\n",
    "  - `duration`, czas trwania ostatniego telefonu reklamowego; autorzy sugerują usunięcie w opisie zbioru, bo nie znamy tej wartości przed wykonaniem telefonu\n",
    "  - `pdays`, liczba dni od ostatniego telefonu reklamowego w ramach danej kampanii marketingowej; jeżeli to pierwszy kontakt, to wartość to 999, i ciężko byłoby włączyć taką cechę do modelu, a mamy już i tak informację o tym, czy to pierwszy kontakt z klientem w zmiennej `previous`\n",
    "  - `poutcome`, wynik poprzedniej kampanii; w zdecydowanej większości przypadków to `nonexistent`\n",
    "3. Dokonaj filtrowania wierszy:\n",
    "  - usuń wiersze z `education` na poziomie `illiterate`, jest ich tylko kilkanaście\n",
    "4. Zakoduj odpowiednio zmienne `education`, `contact`, `month`, `day_of_week` i `y`. Dla ułatwienia słowniki są w zmiennych poniżej.\n",
    "5. Wyodrębnij kolumnę `y` do zmiennej `y` (pamiętaj o usunięciu jej z DataFrame'a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = {\n",
    "    \"basic.4y\": \"primary\",\n",
    "    \"basic.6y\": \"primary\",\n",
    "    \"basic.9y\": \"primary\",\n",
    "    \"high.school\": \"secondary\",\n",
    "    \"professional.course\": \"secondary\",\n",
    "    \"university.degree\": \"tertiary\",\n",
    "}\n",
    "\n",
    "contact_mapping = {\n",
    "    \"telephone\": 0,\n",
    "    \"cellular\": 1,\n",
    "}\n",
    "\n",
    "month_mapping = {\n",
    "    \"jan\": 1,\n",
    "    \"feb\": 2,\n",
    "    \"mar\": 3,\n",
    "    \"apr\": 4,\n",
    "    \"may\": 5,\n",
    "    \"jun\": 6,\n",
    "    \"jul\": 7,\n",
    "    \"aug\": 8,\n",
    "    \"sep\": 9,\n",
    "    \"oct\": 10,\n",
    "    \"nov\": 11,\n",
    "    \"dec\": 12,\n",
    "}\n",
    "\n",
    "day_of_week_mapping = {\n",
    "    \"mon\": 1,\n",
    "    \"tue\": 2,\n",
    "    \"wed\": 3,\n",
    "    \"thu\": 4,\n",
    "    \"fri\": 5,\n",
    "}\n",
    "\n",
    "y_mapping = {\n",
    "    \"no\": 0,\n",
    "    \"yes\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 8 (0.5 punktu)**\n",
    "\n",
    "*Exploratory Data Analysis (EDA)*\n",
    "\n",
    "1. Sprawdź, czy są jakieś wartości brakujące za pomocą biblioteki `missingno`. Jeżeli tak, to sprawdź w dokumentacji zbioru, jaka byłaby sensowna wartość do ich uzupełnienia.\n",
    "2. Narysuj wykres (bar plot) z częstością klas. Uwzględnij częstość na wykresie ([to może się przydać](https://stackoverflow.com/a/68107610/9472066)). Pamiętaj o tytule i opisaniu osi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, będziemy tu mieli do czynienia z problemem klasyfikacji niezbalansowanej. Na szczęście funkcja kosztu w regresji logistycznej pozwala na dodanie **wag klas (class weights)**, aby przypisać większą wagę interesującej nas klasie pozytywnej. Scikit-learn dla wartości `class_weights=\"balanced\"` obliczy wagi odwrotnie proporcjonalne do częstości danej klasy w zbiorze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 9 (1 punkt)**\n",
    "\n",
    "*Podział i preprocessing danych*\n",
    "\n",
    "1. Dokonaj podziału zbioru na treningowy i testowy w proporcjach 75%-25%. Pamiętaj o użyciu podziału ze stratyfikacją (argument `stratify`), aby zachować proporcje klas. Ustaw `random_state=0`.\n",
    "2. Stwórz `ColumnTransformer`, przetwarzający zmienne kategoryczne za pomocą `OneHotEncoder` (teraz już nie musimy robić `drop=\"first\"`), a numeryczne za pomocą `StandardScaler`. Zaaplikuj go do odpowiednich kolumn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 10 (2.5 punktu)**\n",
    "\n",
    "*Trening, tuning i analiza modeli*\n",
    "\n",
    "1. Wytrenuj podstawowy model regresji logistycznej z użyciem `LogisticRegression`. Użyj wag klas (`class_weights=\"balanced\"`). Przetestuj model, wypisując AUROC w procentach. **Uwaga:** Scikit-learn domyślnie stosuje tutaj regularyzację L2, więc przekaż `penalty=\"None\"`.\n",
    "2. Dokonaj tuningu modelu z regularyzacją L2 za pomocą `LogisticRegressionCV`:\n",
    "  - sprawdź 100 wartości, wystarczy podać liczbę do `Cs`\n",
    "  - użyj 5-krotnej walidacji skrośnej\n",
    "  - wybierz najlepszy model według metryki AUROC (parametr `scoring`)\n",
    "  - pamiętaj o `class_weights=\"balanced\"` i `random_state=0`\n",
    "  - użyj `n_jobs=-1` dla przyspieszenia obliczeń\n",
    "  - przetestuj model, wypisując AUROC w procentach\n",
    "  - **uwaga:** Scikit-learn stosuje tutaj konwencję, gdzie parametr `C` to odwrotność siły regularyzacji - im mniejszy, tym silniejsza regularyzacja.\n",
    "3. Dokonaj analogicznego tuningu, ale dla regularyzacji L1. Użyj solwera SAGA. Przetestuj model, wypisując AUROC w procentach.\n",
    "4. Dokonaj analizy wytrenowanych modeli:\n",
    "  - narysuj poziomy wykres słupkowy 10 najważniejszych cech przy użyciu regularyzacji L2, pamiętaj o opisaniu nazw cech, osi i tytule; zwróć uwagę, że wektor współczynników musi być jednowymiarowy\n",
    "  - wypisz, ile procent cech zostało wyeliminowanych przy użyciu regularyzacji L1, oraz wypisz posortowaną listę nazw tych cech\n",
    "  - dokonaj interpretacji 2 wybranych cech spośród najlepszych oraz 2 wybranych spośród tych wyeliminowanych - czy ma to twoim zdaniem sens?\n",
    "  - oblicz AUROC na zbiorze treningowym modelu bez żadnej regularyzacji i porównaj go z wynikiem testowym; czy występuje tutaj overfitting?\n",
    "  - czy twoim zdaniem tworzenie modeli z regularyzacją ma sens w tym przypadku?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 11 (1.5 punktu)**\n",
    "\n",
    "*Dodanie cech wielomianowych do regresji logistycznej*\n",
    "\n",
    "1. Stwórz nowy pipeline do przetwarzania danych do regresji logistycznej, dodając `PolynomialFeatures` do zmiennych numerycznych przed standaryzacją. Wygeneruj cechy o stopniu 2, interakcje oraz potęgi, nie generuj interceptu.\n",
    "2. Wytrenuj model regresji logistycznej bez regularyzacji na takim powiększonym zbiorze. Wypisz AUROC treningowy oraz testowy w procentach.\n",
    "3. Zdecyduj, czy jest sens tworzyć modele z regularyzacją. Jeżeli tak, to wytrenuj i dokonaj tuningu takich modeli. Jeżeli nie, to uzasadnij czemu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dodatkowe (3 punkty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z formalnego, statystycznego punktu widzenia regresja liniowa czyni szereg założeń ([Wikipedia](https://en.wikipedia.org/wiki/Linear_regression#Assumptions)):\n",
    "1. Liniowość - relacja w danych może być reprezentowana jako `y=Xw`.\n",
    "2. Normalność błędów - błędy (rezydua) mają rozkład normalny, wycentrowany na zerze.\n",
    "3. Homoskedastyczność (stała wariancja) - wariancja błędu nie zależy od wartości docelowych `y`. Innymi słowy, nasz błąd będzie w przybliżeniu miał podobny \"rozrzut\" dla małych i dużych wartości `y`.\n",
    "4. Niezależność błędów - błąd i `y` są niezależne (w sensie statystycznym). Innymi słowy, nie ma między nimi bezpośredniej relacji. Jeżeli nie pracujemy z szeregami czasowymi, to to założenie po prostu jest spełnione.\n",
    "5. Brak współliniowości zmiennych - nie ma idealnej korelacji cech.\n",
    "\n",
    "Testowanie tych własności nie zawsze jest oczywiste, a w szczególności Scikit-learn oferuje tutaj dość mało opcji, bo pochodzą one głównie z tradycyjnej statystyki.\n",
    "\n",
    "1. Liniowość:\n",
    "  - numerycznie: wysoki współczynnik dopasowania modelu $R^2$ na zbiorze treningowym, niski błąd (RMSE) na zbiorze treningowym oraz testowym\n",
    "  - testem statystycznym: [Rainbow test](https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.linear_rainbow.html) lub [Harvey Collier test](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.linear_harvey_collier.html)\n",
    "  - graficznie: możliwe kiedy mamy 1/2 zmienne i da się narysować wykres zmiennej zależnej względem cech\n",
    "2. Normalność błędów:\n",
    "  - graficznie: robimy histogram rezyduów, powinien mieć kształt rozkładu normalnego i być wycentrowany na zerze\n",
    "  - testem statystycznym: [Jarque-Bera test](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test), [Omnibus normality test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html)\n",
    "3. Homoskedastyczność:\n",
    "  - graficznie: robimy scatter plot rezyduów dla wartości przewidywanych od najmniejszej do największej, nie powinno być na nim żadnych widocznych wzorców czy kształtów; [przykład 1](https://towardsdatascience.com/multivariant-linear-regression-e636a4f99b40), [przykład 2](https://www.vexpower.com/brief/homoskedasticity)\n",
    "  - testem statystycznym: [Breusch–Pagan test](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) lub [Goldfeld-Quandt test](https://en.wikipedia.org/wiki/Goldfeld%E2%80%93Quandt_test)\n",
    "4. Niezależność błędów - nie omawiam, bo dotyczy tylko szeregów czasowych.\n",
    "5. Brak współliniowości zmiennych: numerycznie, sprawdzić korelacje zmiennych, lub współczynnik uwarunkowania macierzy `X`\n",
    "\n",
    "\n",
    "W ramach zadania wytrenuj model regresji liniowej dla zbioru danych Ames Housing z użyciem biblioteki Statsmodels: [OLS docs](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html), [OLS](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html), [Regression diagnostics](https://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html). Wytrenuj najpierw model bez regularyzacji, a następnie z regularyzacją L2 oraz L1. Nie przeprowadzaj tuningu, użyj tych wartości siły regularyzacji, które wyznaczyliśmy wcześniej.\n",
    "\n",
    "Przetestuj założenia za pomocą testów statystycznych: Harvey Collier, Jarque-Bera, Breusch–Pagan. Współliniowość zmiennych zweryfikuj z użyciem współczynnika uwarunkowania. Zastosuj poziom istotności $\\alpha=0.05$.\n",
    "\n",
    "Czy założenia są spełnione w przypadku podstawowego modelu i/lub modeli z regularyzacją? Czy modele regularyzowane w lepszym stopniu spełniają założenia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUM",
   "language": "python",
   "name": "pum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
